{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "! pip install transformers\n",
        "! pip install sentence_transformers\n",
        "! pip install -U sentence-transformers\n",
        "# ! pip install numpy\n",
        "! pip install torch\n",
        "! pip install torchtext\n",
        "! pip install torchmetrics\n",
        "! pip install pytorch-lightning\n",
        "! pip install time\n",
        "! pip install ipykernel\n",
        "! pip install spacy\n",
        "! pip install \"grpcio>=1.37.0,<2.0\" \"h5py>=3.6.0,<3.7\" \"numpy>=1.22.3,<1.23.0\""
      ],
      "metadata": {
        "id": "lXsvd2hC_W2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "import random\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ab76DlzRXDUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Data:\n",
        "    '''\n",
        "    Class that cleans and formats the bills and news datasets for the BERT\n",
        "    model, tokenizes the data, and creates then saves the text embeddings.\n",
        "    '''\n",
        "    def __init__(self, random_seed = 5,\n",
        "                bert_model = 'bert-base-uncased', #'bert-base-uncased' or 'bert-large-uncased'\n",
        "                date_range_begin = None, date_range_end = '2018-04-01',\n",
        "                bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th.csv',\n",
        "                clean_bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_clean.csv',\n",
        "                minimal_clean_bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_clean_minimal.csv',\n",
        "                foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox.csv',\n",
        "                clean_foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox_clean.csv',\n",
        "                minimal_clean_foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox_clean_minimal.csv',\n",
        "                breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart.csv',\n",
        "                clean_breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_clean.csv',\n",
        "                minimal_clean_breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_clean_minimal.csv',\n",
        "                cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn.csv',\n",
        "                clean_cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_clean.csv',\n",
        "                minimal_clean_cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_clean_minimal.csv',\n",
        "                nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt.csv',\n",
        "                clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean.csv',\n",
        "                minimal_clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal.csv',\n",
        "                truncated_minimal_clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal_truncated.csv',\n",
        "                wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date.csv',\n",
        "                clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean.csv',\n",
        "                minimal_clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal.csv',\n",
        "                truncated_minimal_clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal_truncated.csv'\n",
        "    ):\n",
        "        self.device = self.cuda_mps_cpu()\n",
        "        self.random_seed = random_seed\n",
        "        self.random_seed_function()\n",
        "        self.date_range_begin = date_range_begin\n",
        "        self.date_range_end = date_range_end\n",
        "        self.bills_csvpath = bills_csvpath\n",
        "        self.clean_bills_csvpath = clean_bills_csvpath\n",
        "        self.minimal_clean_bills_csvpath = minimal_clean_bills_csvpath\n",
        "        self.df_bills_prepared = pd.read_csv(minimal_clean_bills_csvpath)\n",
        "        # self.df_bills_raw = pd.read_csv(bills_csvpath)\n",
        "        # self.df_bills_clean = self.clean_bills()\n",
        "        # self.df_bills_clean = pd.read_csv(clean_bills_csvpath)\n",
        "        self.foxnews_csvpath = foxnews_csvpath\n",
        "        self.minimal_clean_foxnews_csvpath = minimal_clean_foxnews_csvpath\n",
        "        self.breitbart_csvpath = breitbart_csvpath\n",
        "        self.minimal_clean_breitbart_csvpath = minimal_clean_breitbart_csvpath\n",
        "        self.cnn_csvpath = cnn_csvpath\n",
        "        self.minimal_clean_cnn_csvpath = minimal_clean_cnn_csvpath\n",
        "        self.nytimes_csvpath = nytimes_csvpath\n",
        "        self.minimal_clean_nytimes_csvpath = minimal_clean_nytimes_csvpath\n",
        "        self.truncated_minimal_clean_nytimes_csvpath = truncated_minimal_clean_nytimes_csvpath\n",
        "        self.wapo_csvpath = wapo_csvpath\n",
        "        self.minimal_clean_wapo_csvpath = minimal_clean_wapo_csvpath\n",
        "        self.truncated_minimal_clean_wapo_csvpath = truncated_minimal_clean_wapo_csvpath\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "        self.bert_base = AutoModel.from_pretrained(bert_model)\n",
        "        self.practice_bill = self.df_bills_prepared.head(5).copy()\n",
        "\n",
        "\n",
        "    def clean_bills(self, only_2017 = False, only_2018 = False,\n",
        "                save = True # only_bills = False,\n",
        "    ):\n",
        "        '''\n",
        "        Clean and format the bills dataset\n",
        "        '''\n",
        "        df = pd.read_csv(self.bills_csvpath)\n",
        "        df.loc[:, ['new_index']] = df.index\n",
        "        df.loc[:, 'cleaned_text'] = df.loc[:, 'raw_text'].apply(\n",
        "                self.clean_bill_text, args=()\n",
        "        )\n",
        "        df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'introduced_date'], format='%Y-%m-%d'\n",
        "        )\n",
        "        df.loc[:, ['house_passage_binary']] = df.loc[:, 'house_passage'].fillna(0, inplace=True)\n",
        "\n",
        "        df.loc[:, ['house_passage_binary']] = np.where(\n",
        "                df.loc[:, 'house_passage_binary'] != 0, 1, 0\n",
        "        )\n",
        "\n",
        "        if only_2017:\n",
        "            df = df.loc[(df.loc[:, 'date'] >= '2017-01-01'\n",
        "                    & df.loc[:, 'date'] < '2018-01-01'\n",
        "            ), :]\n",
        "\n",
        "        if only_2018:\n",
        "            df = df.loc[(df.loc[:, 'date'] >= '2018-01-01'\n",
        "                    & df.loc[:, 'date'] < '2019-01-01'\n",
        "            ), :]\n",
        "\n",
        "        df.loc[:, ['cleaned_text']] = df.loc[:, 'cleaned_text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df = df.loc[:, ['bill_id', 'new_index', 'cleaned_text', 'date', 'house_passage_binary', 'bill_type']]\n",
        "\n",
        "        if save:\n",
        "            df.to_csv(self.clean_bills_csvpath, index=False)\n",
        "        # if only_bills:\n",
        "        #     df = df.loc[(df.loc[:, 'bill_type'] == | df.loc[:, 'bill_type'] == ), :]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def dates_clean_news(self, df_whole, Date_version, long_date_version,\n",
        "            start_date = None, end_date = None,\n",
        "            minimal_columns = False\n",
        "            # minimal_columns = ['index', 'date', 'cleaned_text']\n",
        "    ):\n",
        "        '''\n",
        "        Format the date of the news articles to match the date of the bills\n",
        "        '''\n",
        "        if start_date is None:\n",
        "            start_date = self.date_range_begin\n",
        "        if end_date is None:\n",
        "            end_date = self.date_range_end\n",
        "\n",
        "        df = df_whole.copy()\n",
        "        if Date_version:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'Date'], format='%Y-%m-%d'\n",
        "            )\n",
        "        elif long_date_version:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'date'].str[:10], format='%Y-%m-%d'\n",
        "            )\n",
        "        else:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'date'], format='%Y-%m-%d'\n",
        "            )\n",
        "\n",
        "        if start_date is not None:\n",
        "            df = df.loc[df.loc[:, 'date'] >= start_date, :]\n",
        "        if end_date is not None:\n",
        "            df = df.loc[df.loc[:, 'date'] <= end_date, :]\n",
        "\n",
        "        if minimal_columns is not False:\n",
        "            df = df.loc[:, minimal_columns]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def clean_foxnews(self, save = True):\n",
        "        '''\n",
        "        Clean and format the Fox News data\n",
        "        '''\n",
        "        df_fox = pd.read_csv(self.foxnews_csvpath)\n",
        "        df_fox.loc[:, ['new_index']] = df_fox.index\n",
        "        df_fox_dated = self.dates_clean_news(df_fox, Date_version = True,\n",
        "                long_date_version = False\n",
        "        )\n",
        "        df_fox_dated.loc[:, 'cleaned_text'] = df_fox_dated.loc[:, 'article_text'].apply(\n",
        "                self.clean_foxnews_text, args=()\n",
        "        )\n",
        "        df_fox_dated = df_fox_dated.loc[:, ['uuid', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_fox_dated.to_csv(self.minimal_clean_foxnews_csvpath, index=False)\n",
        "\n",
        "        return df_fox_dated\n",
        "\n",
        "\n",
        "    def clean_breitbart(self, save = True):\n",
        "        '''\n",
        "        Clean and format the Breitbart data\n",
        "        '''\n",
        "        df_breitbart = pd.read_csv(self.breitbart_csvpath)\n",
        "        df_breitbart.loc[:, ['new_index']] = df_breitbart.index\n",
        "        df_breitbart_dated = self.dates_clean_news(df_breitbart,\n",
        "                Date_version = True, long_date_version = False\n",
        "        )\n",
        "        df_breitbart_dated.loc[:, 'cleaned_text'] = df_breitbart_dated.loc[:, 'article_text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_breitbart_dated = df_breitbart_dated.loc[:, ['uuid', 'new_index', 'cleaned_text', 'date']]\n",
        "        # df_breitbart_dated.replace('', np.nan, inplace=True)\n",
        "        # df_breitbart_dated.dropna(subset= ['cleaned_text'], inplace=True)\n",
        "\n",
        "        if save:\n",
        "            df_breitbart_dated.to_csv(self.minimal_clean_breitbart_csvpath,\n",
        "                    index=False\n",
        "            )\n",
        "\n",
        "        return df_breitbart_dated\n",
        "\n",
        "\n",
        "    def clean_cnn(self, save = True):\n",
        "        '''\n",
        "        Clean and format the CNN data\n",
        "        '''\n",
        "        df_cnn = pd.read_csv(self.cnn_csvpath)\n",
        "        df_cnn.loc[:, ['new_index']] = df_cnn.index\n",
        "        df_cnn_dated = self.dates_clean_news(df_cnn, Date_version = False,\n",
        "                long_date_version = True\n",
        "        )\n",
        "        df_cnn_dated.loc[:, 'cleaned_text'] = df_cnn_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_cnn_dated = df_cnn_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_cnn_dated.to_csv(self.minimal_clean_cnn_csvpath, index=False)\n",
        "\n",
        "        return df_cnn_dated\n",
        "\n",
        "\n",
        "    def clean_nytimes(self, save = True):\n",
        "        '''\n",
        "        Clean and format the New York Times data\n",
        "        '''\n",
        "        df_nytimes = pd.read_csv(self.nytimes_csvpath)\n",
        "        df_nytimes.loc[:, ['new_index']] = df_nytimes.index\n",
        "        df_nytimes_dated = self.dates_clean_news(df_nytimes,\n",
        "                Date_version = False, long_date_version = True\n",
        "        )\n",
        "        df_nytimes_dated.loc[:, 'cleaned_text'] = df_nytimes_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_nytimes_dated = df_nytimes_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_nytimes_dated.to_csv(self.minimal_clean_nytimes_csvpath,\n",
        "                    index=False\n",
        "            )\n",
        "\n",
        "        return df_nytimes_dated\n",
        "\n",
        "\n",
        "    def clean_wapo(self, save = True):\n",
        "        '''\n",
        "        Clean and format the Washington Post data\n",
        "        '''\n",
        "        df_wapo = pd.read_csv(self.wapo_csvpath)\n",
        "        df_wapo.loc[:, ['new_index']] = df_wapo.index\n",
        "        df_wapo_dated = self.dates_clean_news(df_wapo, Date_version = False,\n",
        "                long_date_version = False\n",
        "        )\n",
        "        df_wapo_dated.loc[:, 'cleaned_text'] = df_wapo_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_wapo_dated = df_wapo_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_wapo_dated.to_csv(self.minimal_clean_wapo_csvpath, index=False)\n",
        "\n",
        "        return df_wapo_dated\n",
        "\n",
        "\n",
        "    def clean_bill_text(self, text, glove=None, cbows=None, lemma=None):\n",
        "        '''\n",
        "        Intakes the text of a single bill and removes formating unique to the\n",
        "        bill text, eliminates html tags, separates sentences,\n",
        "        lower cases, and correctly formats the text\n",
        "        '''\n",
        "        text = text.lower()\n",
        "        text = re.compile('<.*?>').sub('', text)\n",
        "        text = re.compile(r\"[_]\").sub(\" \", text)\n",
        "        text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        text = re.sub(r\"\\d+\\sU\\.S\\.C\\.\\s\\d+[a-z]?(\\(\\d+\\))?( \\([a-z]+\\))?\", \"\",\n",
        "                text\n",
        "        )\n",
        "        text = re.sub(r'(sec\\.\\s+\\d+\\.?)|(section\\s+\\d+\\.)', '', text)\n",
        "        text = re.sub(r'\\(\\d+ u\\.s\\.c\\. \\d+\\([a-z]\\)(\\(\\d+\\))*\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+\\s+u\\.s\\.c\\.\\s+\\d+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+ u\\.s\\.c\\. \\d+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+\\)', '', text)\n",
        "        text = re.sub(r'\\([ivxlcdm]+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\w\\)', '', text)\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'([a-z])\\.--([a-z])', r'\\1. -- \\2', text)\n",
        "        text = text.replace('``', '\"').replace(\"''\", '\"')\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_foxnews_text(self, text):\n",
        "        '''\n",
        "        Intakes a single article and removes formating unique to the Fox News\n",
        "        text, lower cases, and correctly formating the text\n",
        "        '''\n",
        "        text = text.replace(\"Get the latest updates from the 2024 campaign trail, exclusive interviews and more Fox News politics content.SubscribedYou've successfully subscribed to this newsletter!\", \" \")\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"([.!?;:\\\"“”])(?=[^\\s])|”(?=\\w)\", r\"\\1 \", text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_generalnews_text(self, text):\n",
        "        '''\n",
        "        Intakes a single article and removes formating unique to the General\n",
        "        News text, lower cases, separates connected end of sentences,\n",
        "        and correctly formating the text\n",
        "        '''\n",
        "        text = re.sub(r\"http\\S+\", \" \", text)\n",
        "        text = re.sub(r\"@\\S+\", \" \", text)\n",
        "        text = re.sub(r'([a-z])\\.”([A-Z])', r'\\1. ” \\2', text)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"([.!?;:\\\"“”])(?=[^\\s])|”(?=\\w)\", r\"\\1 \", text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def long_texts_dim_reduction(self, embedding_tensor,\n",
        "                dim_reduction_strategy\n",
        "    ): # Pooling, max, (potential: PCA)\n",
        "        '''\n",
        "        If tokenized text is untruncated and longer than 510 tokens, adds a\n",
        "        secondary measure to reduce the dimensionality of the embedding\n",
        "        tensor\n",
        "        '''\n",
        "        if dim_reduction_strategy == 'mean':\n",
        "            embedding_tensor = embedding_tensor.mean(dim=0)\n",
        "        elif dim_reduction_strategy == 'max':\n",
        "            embedding_tensor = embedding_tensor.max(dim=0).values\n",
        "\n",
        "        print(embedding_tensor.shape)\n",
        "        return embedding_tensor\n",
        "\n",
        "\n",
        "    def bert_embed_single_row(self, text, index = None, pooling = 'mean',\n",
        "            max_len = 510, attention_mask_onpadding = True,\n",
        "            long_dim_reduction_strategy = 'max',\n",
        "            skip_long_texts = False, truncate_text = False,\n",
        "    ): #512 tokens BERT # max, mean, CLS token embedding\n",
        "        '''\n",
        "        Intakes an entire bill or article, tokenizens the text, embeds the text\n",
        "        using the BERT model, applies pooling, and returns the embedding tensor.\n",
        "        Text may be truncated to max_len tokens.\n",
        "        '''\n",
        "        print(f'Bill/News index: {index}')\n",
        "        if truncate_text:\n",
        "            text_tokenized = self.tokenizer(text, return_tensors='pt',\n",
        "                add_special_tokens=False, truncation=True, max_length = max_len\n",
        "            )\n",
        "        else:\n",
        "            text_tokenized = self.tokenizer(text, return_tensors='pt',\n",
        "                add_special_tokens=False, truncation=False\n",
        "            )\n",
        "        text_len = len(text_tokenized.input_ids[0])\n",
        "        if skip_long_texts:\n",
        "            if text_len > max_len:\n",
        "                return None\n",
        "        # print(text_tokenized.input_ids[0])\n",
        "        text_segment_count = math.ceil(text_len / max_len)\n",
        "        embedding_tensor = torch.tensor([], dtype = torch.float32)\n",
        "        for segment in range(text_segment_count):\n",
        "            start_token = segment * max_len\n",
        "            end_token = min(start_token + max_len, text_len)\n",
        "            # print(f'Segment {segment}, start_token {start_token}, end_token {end_token}')\n",
        "            # print(self.tokenizer.cls_token_id, self.tokenizer.sep_token_id)\n",
        "            segment_ids_tensor = text_tokenized.input_ids[0, start_token:end_token]\n",
        "            padded_token_count = (max_len) - (end_token - start_token)\n",
        "            if padded_token_count == 0:\n",
        "                segment_ids_tensor = torch.cat([\n",
        "                        torch.tensor([self.tokenizer.cls_token_id]),\n",
        "                        segment_ids_tensor,\n",
        "                        torch.tensor([self.tokenizer.sep_token_id])\n",
        "                ]).unsqueeze(0)\n",
        "            else:\n",
        "                padding_tokens_tensor = torch.tensor(\n",
        "                        [self.tokenizer.pad_token_id] * padded_token_count\n",
        "                )\n",
        "                segment_ids_tensor = torch.cat([\n",
        "                        torch.tensor([self.tokenizer.cls_token_id]),\n",
        "                        segment_ids_tensor,\n",
        "                        padding_tokens_tensor,\n",
        "                        torch.tensor([self.tokenizer.sep_token_id])\n",
        "                ]).unsqueeze(0)\n",
        "            if attention_mask_onpadding:\n",
        "                attention_mask = (segment_ids_tensor !=\n",
        "                        self.tokenizer.pad_token_id).to(dtype=torch.int64\n",
        "                )\n",
        "                with torch.no_grad():\n",
        "                    segment_embeddings_tensor = self.bert_base(\n",
        "                            input_ids = segment_ids_tensor,\n",
        "                            attention_mask=attention_mask\n",
        "                    ).last_hidden_state\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    segment_embeddings_tensor = self.bert_base(\n",
        "                            segment_ids_tensor\n",
        "                    ).last_hidden_state\n",
        "\n",
        "            if segment == 0:\n",
        "                total_embeddings_tensor = segment_embeddings_tensor\n",
        "            else:\n",
        "                # print('total_embeddings_tensor: ', total_embeddings_tensor.shape)\n",
        "                # print('segment_embeddings_tensor: ', segment_embeddings_tensor.shape)\n",
        "                total_embeddings_tensor = torch.cat([total_embeddings_tensor,\n",
        "                        segment_embeddings_tensor\n",
        "                ])\n",
        "\n",
        "        if pooling == 'mean':\n",
        "            output_embedding_tensor = total_embeddings_tensor.mean(dim=1)\n",
        "        elif pooling == 'max':\n",
        "            output_embedding_tensor = total_embeddings_tensor.max(dim=1).values\n",
        "        else: # None or CLS implies CLS\n",
        "            output_embedding_tensor = total_embeddings_tensor[:, 0, :]\n",
        "\n",
        "        if not skip_long_texts:\n",
        "            if not truncate_text:\n",
        "                output_embedding_tensor = self.long_texts_dim_reduction(\n",
        "                        output_embedding_tensor, long_dim_reduction_strategy\n",
        "                )\n",
        "\n",
        "        print('output_embedding_tensor: ', output_embedding_tensor.shape)\n",
        "        return output_embedding_tensor\n",
        "\n",
        "\n",
        "    def total_bert_embeddings(self, df, text_column, pooling = 'mean',\n",
        "            max_len = 510, attention_mask_onpadding = True,\n",
        "            long_dim_reduction_strategy = 'max',\n",
        "            skip_long_texts = False, truncate_text = False\n",
        "    ):\n",
        "        '''\n",
        "        Intakes a dataframe, facilitates the tokenization and embedding process\n",
        "        using the BERT model for each row in the dataframe.  Returns the\n",
        "        total embedding tensor.  Text may be truncated to max_len tokens.\n",
        "        '''\n",
        "        embeddings_lst = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            ## Used in testing\n",
        "            # if index == 10:\n",
        "            #     break\n",
        "            embedded_row = self.bert_embed_single_row(row.loc[text_column],\n",
        "                            index, pooling, max_len, attention_mask_onpadding,\n",
        "                            long_dim_reduction_strategy, skip_long_texts,\n",
        "                            truncate_text\n",
        "            )\n",
        "            # If skip_long_texts is True and text after tokenization longer than 512 than text skipped\n",
        "            if embedded_row is not None:\n",
        "                embeddings_lst.append(embedded_row)\n",
        "\n",
        "        embeddings_tensor = torch.stack(embeddings_lst)\n",
        "\n",
        "        return embeddings_tensor\n",
        "\n",
        "\n",
        "    def cuda_mps_cpu(self):\n",
        "        '''\n",
        "        States the computational device in use\n",
        "        '''\n",
        "        if torch.cuda.is_available():  # use GPU if available\n",
        "            print('Using GPU')\n",
        "            return torch.device('cuda')\n",
        "        # https://www.linkedin.com/pulse/how-use-gpu-tensorflow-pytorch-libraries-macbook-pro-m2apple-kashyap/\n",
        "        elif torch.backends.mps.is_available():\n",
        "            print('Using MPS')\n",
        "            return torch.device('mps')\n",
        "        else:\n",
        "            print('Using CPU')\n",
        "            return torch.device('cpu')\n",
        "\n",
        "\n",
        "    def random_seed_function(self):\n",
        "        '''\n",
        "        Sets the random seed for replicability\n",
        "        '''\n",
        "        torch.manual_seed(self.random_seed)\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.manual_seed_all(self.random_seed)\n",
        "\n",
        "        random.seed(self.random_seed)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "23pX4NW__lm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_class = BERT_Data()"
      ],
      "metadata": {
        "id": "xnNzrYypZWZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_class.clean_bills()"
      ],
      "metadata": {
        "id": "P22mpjv9om_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyt_df = pd.read_csv(data_class.minimal_clean_nytimes_csvpath)\n",
        "\n",
        "print(f'length = {len(nyt_df)}')\n",
        "\n",
        "truncated_nyt_df = nyt_df.sample(n = 20000, random_state = data_class.random_seed).copy()\n",
        "\n",
        "truncated_nyt_df\n",
        "\n",
        "print(f'length = {len(truncated_nyt_df)}')\n",
        "\n",
        "truncated_nyt_df.loc[:, 'nytimes_truncated_new_index'] = list(range(len(truncated_nyt_df)))\n",
        "\n",
        "truncated_nyt_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal_truncated.csv')\n"
      ],
      "metadata": {
        "id": "Jn_Y9QOa951k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wapo_df = pd.read_csv(data_class.minimal_clean_wapo_csvpath)\n",
        "\n",
        "print(f'length = {len(wapo_df)}')\n",
        "\n",
        "truncated_wapo_df = wapo_df.sample(n = 20000, random_state = data_class.random_seed).copy()\n",
        "\n",
        "print(f'length = {len(truncated_wapo_df)}')\n",
        "\n",
        "truncated_wapo_df.loc[:, 'wapo_truncated_new_index'] = list(range(len(truncated_wapo_df)))\n",
        "\n",
        "truncated_wapo_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal_truncated.csv')\n"
      ],
      "metadata": {
        "id": "vKJP0vHTSse2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bills = pd.read_csv(data_class.clean_bills_csvpath)\n",
        "df_foxnews = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "df_breitbart = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "df_cnn = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "df_nytimes =  pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "df_wapo = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)"
      ],
      "metadata": {
        "id": "wIl8uOShAL_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truncated_wapo_df.head(2)"
      ],
      "metadata": {
        "id": "_shSNVZZ_xwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_dfcolumns_lst = ['new_index', 'bill_index', 'bill_index_2017-01', 'bill_index_2017-02', 'bill_index_2017-03', 'bill_index_2017-04', 'bill_index_2017-05', 'bill_index_2017-06', 'bill_index_2017-07', 'bill_index_2017-08', 'bill_index_2017-09', 'bill_index_2017-10', 'foxnews_index', 'foxnews_index_2016-11', 'foxnews_index_2016-12', 'foxnews_index_2017-01', 'foxnews_index_2017-02', 'foxnews_index_2017-03', 'foxnews_index_2017-04', 'foxnews_index_2017-05', 'foxnews_index_2017-06', 'foxnews_index_2017-07', 'foxnews_index_2017-08', 'foxnews_index_2017-09', 'foxnews_index_2017-10', 'foxnews_index_2017-11', 'foxnews_index_2017-12', 'breitbart_index', 'breitbart_index_2016-11', 'breitbart_index_2016-12', 'breitbart_index_2017-01', 'breitbart_index_2017-02', 'breitbart_index_2017-03', 'breitbart_index_2017-04', 'breitbart_index_2017-05', 'breitbart_index_2017-06', 'breitbart_index_2017-07', 'breitbart_index_2017-08', 'breitbart_index_2017-09', 'breitbart_index_2017-10', 'breitbart_index_2017-11', 'breitbart_index_2017-12', 'cnn_index', 'cnn_index_2016-11', 'cnn_index_2016-12', 'cnn_index_2017-01', 'cnn_index_2017-02', 'cnn_index_2017-03', 'cnn_index_2017-04', 'cnn_index_2017-05', 'cnn_index_2017-06', 'cnn_index_2017-07', 'cnn_index_2017-08', 'cnn_index_2017-09', 'cnn_index_2017-10', 'cnn_index_2017-11', 'cnn_index_2017-12', 'nytimes_index', 'nytimes_index_2016-11', 'nytimes_index_2016-12', 'nytimes_index_2017-01', 'nytimes_index_2017-02', 'nytimes_index_2017-03', 'nytimes_index_2017-04', 'nytimes_index_2017-05', 'nytimes_index_2017-06', 'nytimes_index_2017-07', 'nytimes_index_2017-08', 'nytimes_index_2017-09', 'nytimes_index_2017-10',  'nytimes_index_2017-11', 'nytimes_index_2017-12', 'wapo_index', 'wapo_index_2016-11', 'wapo_index_2016-12', 'wapo_index_2017-01', 'wapo_index_2017-02', 'wapo_index_2017-03', 'wapo_index_2017-04', 'wapo_index_2017-05', 'wapo_index_2017-06', 'wapo_index_2017-07', 'wapo_index_2017-08', 'wapo_index_2017-09', 'wapo_index_2017-10', 'wapo_index_2017-11', 'wapo_index_2017-12']\n",
        "\n",
        "df_column_ranges = [len(pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "        ), len(pd.read_csv(data_class.minimal_clean_foxnews_csvpath)), len(\n",
        "        pd.read_csv(data_class.minimal_clean_breitbart_csvpath)), len(\n",
        "        pd.read_csv(data_class.minimal_clean_cnn_csvpath)), len(\n",
        "        pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)), len(\n",
        "        pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath))]\n",
        "\n",
        "rows_max = 20000\n",
        "\n",
        "master_index_df = pd.DataFrame(np.full((rows_max, len(index_dfcolumns_lst)), np.nan),\n",
        "    columns = index_dfcolumns_lst\n",
        ")\n",
        "\n",
        "master_index_df.loc[:, 'new_index'] = list(range(len(master_index_df)))"
      ],
      "metadata": {
        "id": "jnbfx_ln3x0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranges_index = 0\n",
        "\n",
        "for index, column_name in enumerate(index_dfcolumns_lst):\n",
        "    if index > 0:\n",
        "        if '_index' == column_name[-6:]:\n",
        "            column_index_len = df_column_ranges[ranges_index]\n",
        "            nan_lst_len = rows_max - column_index_len\n",
        "            master_index_df.loc[:, column_name] = list(\n",
        "                    range(df_column_ranges[ranges_index])) + ([np.nan] *\n",
        "                    nan_lst_len\n",
        "            )\n",
        "            ranges_index += 1"
      ],
      "metadata": {
        "id": "maOES0-o_rQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bills.columns"
      ],
      "metadata": {
        "id": "6PgDfsErkHww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bills.loc[:, ['bill_date']] = df_bills.loc[:, 'date']\n",
        "df_bills.loc[:, ['bill_title']] = df_bills.loc[:, 'title']\n",
        "master_index_df = pd.merge(master_index_df,\n",
        "        df_bills.loc[:,['bill_date', 'bill_slug', 'title', 'new_index']], on = 'new_index',\n",
        "        how = 'left'\n",
        ")"
      ],
      "metadata": {
        "id": "d8sSDvcyjy-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.columns"
      ],
      "metadata": {
        "id": "jiRxDboi5Hws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_foxnews.columns"
      ],
      "metadata": {
        "id": "srjx4ev4xpxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_foxnews.loc[:, ['foxnews_date']] = df_foxnews.loc[:, 'date']\n",
        "master_index_df = pd.merge(master_index_df,\n",
        "        df_foxnews.loc[:,['foxnews_date', 'new_index']], on = 'new_index',\n",
        "        how = 'left'\n",
        ")"
      ],
      "metadata": {
        "id": "G6MJbyqSxi9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.head(2)"
      ],
      "metadata": {
        "id": "FvU8_9hjU4jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_breitbart.columns"
      ],
      "metadata": {
        "id": "hauHUNAq-R4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_breitbart.loc[:, ['breitbart_date']] = df_breitbart.loc[:, 'date']\n",
        "master_index_df = pd.merge(master_index_df,\n",
        "        df_breitbart.loc[:,['breitbart_date', 'new_index']], on = 'new_index',\n",
        "        how = 'left'\n",
        ")"
      ],
      "metadata": {
        "id": "L9W6XkkH6Nko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn.columns"
      ],
      "metadata": {
        "id": "KJ6opEHU-Vqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn.loc[:, ['cnn_date']] = df_cnn.loc[:, 'date']\n",
        "master_index_df = pd.merge(master_index_df,\n",
        "        df_cnn.loc[:, ['cnn_date', 'new_index']], on = 'new_index',\n",
        "        how = 'left'\n",
        ")"
      ],
      "metadata": {
        "id": "_4Mlmg3j6d6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nytimes.columns"
      ],
      "metadata": {
        "id": "XIgJMTQh-Xyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nytimes.loc[:, ['nytimes_date']] = df_nytimes.loc[:, 'date']\n",
        "df_nytimes.loc[:, ['nytimes_whole_new_index']] = df_nytimes.loc[:, 'new_index']\n",
        "master_index_df = pd.merge(master_index_df,\n",
        "        df_nytimes.loc[:,['nytimes_date', 'nytimes_truncated_new_index', 'nytimes_whole_new_index']], left_on = 'new_index',\n",
        "        right_on = 'nytimes_truncated_new_index', how = 'left'\n",
        ")"
      ],
      "metadata": {
        "id": "zc0EewzR6sWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wapo.columns"
      ],
      "metadata": {
        "id": "y222pBxl-cvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wapo.loc[:, ['wapo_date']] = df_wapo.loc[:, 'date']\n",
        "df_wapo.loc[:, ['wapo_whole_new_index']] = df_wapo.loc[:, 'new_index']\n",
        "master_index_df = pd.merge(master_index_df,\n",
        "        df_wapo.loc[:,['wapo_date', 'wapo_truncated_new_index', 'wapo_whole_new_index']], left_on = 'new_index',\n",
        "        right_on = 'wapo_truncated_new_index', how = 'left'\n",
        ")"
      ],
      "metadata": {
        "id": "qxV_gglY7WZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.columns"
      ],
      "metadata": {
        "id": "Z1uTCzwR_Xx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df"
      ],
      "metadata": {
        "id": "Ur1n_lO_8CAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/bills_data/master_index_df.csv')\n",
        "master_index_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/news_data/master_index_df.csv')"
      ],
      "metadata": {
        "id": "I59DU04HEAoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df = pd.read_csv('/content/drive/Shareddrives/PulseofPolicy_data/bills_data/master_index_df.csv')"
      ],
      "metadata": {
        "id": "XRYncn9VE2a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.columns"
      ],
      "metadata": {
        "id": "jKZrX5AYFRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suffix_lst_news = ['_index', '_date', '_2016-11', '_2016-12', '_2017-01', '_2017-02', '_2017-03', '_2017-04', '_2017-05', '_2017-06', '_2017-07', '_2017-08', '_2017-09', '_2017-10', '_2017-11', '_2017-12']"
      ],
      "metadata": {
        "id": "7t7ggEkmOqZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suffix_lst_news[0] = '_truncated_new' + suffix_lst_news[0]"
      ],
      "metadata": {
        "id": "44QmffkHOudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.loc[:, 'bill_date'][7]\n",
        "Unnamed: 0"
      ],
      "metadata": {
        "id": "yXCHKPr6gHXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.columns"
      ],
      "metadata": {
        "id": "GB6WL5wTQbRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XRxLF3q3P6cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df.head(1)\n"
      ],
      "metadata": {
        "id": "HM3wSPSHQhUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#      Create Master Key Dataframe     #\n",
        "########################################"
      ],
      "metadata": {
        "id": "o7aPxhFPqU7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_index_df = pd.read_csv('/content/drive/Shareddrives/PulseofPolicy_data/bills_data/master_index_df.csv')\n",
        "master_index_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "master_index_df.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
        "print(f'Initial master_index_df shape {master_index_df.shape}')\n",
        "\n",
        "edit_master_index_df = master_index_df.copy()\n",
        "\n",
        "bills_suffix_lst = ['_date', '_index_2017-01', '_index_2017-02', '_index_2017-03', '_index_2017-04', '_index_2017-05', '_index_2017-06', '_index_2017-07', '_index_2017-08', '_index_2017-09', '_index_2017-10']\n",
        "news_suffix_lst = ['_date', '_index_2016-11', '_index_2016-12', '_index_2017-01', '_index_2017-02', '_index_2017-03', '_index_2017-04', '_index_2017-05', '_index_2017-06', '_index_2017-07', '_index_2017-08', '_index_2017-09', '_index_2017-10', '_index_2017-11', '_index_2017-12']\n",
        "\n",
        "\n",
        "sources_lst = ['bill', 'foxnews', 'breitbart', 'cnn', 'nytimes', 'wapo']\n",
        "extract_constants_lst = ['new_index']\n",
        "\n",
        "def column_extraction(df, source, suffix_lst,\n",
        "        extract_constants_lst = extract_constants_lst, truncated = False\n",
        "):\n",
        "    '''\n",
        "    Extract columns from source dataframe based source and suffix list.\n",
        "    Returns both the extracted dataframe and the columns to extract.\n",
        "    '''\n",
        "    df_copy = df.copy()\n",
        "    # print(f'__________ini_columns_to_extract____________: {columns_to_extract}')\n",
        "    columns_to_extract = []\n",
        "    print(f'__________empty_columns_to_extract____________: {columns_to_extract}')\n",
        "    print(f'__________extract_constants_lst____________: {extract_constants_lst}')\n",
        "    columns_to_extract = extract_constants_lst\n",
        "    print(f'__________with_new_index_columns_to_extract____________: {extract_constants_lst}')\n",
        "\n",
        "\n",
        "    for suffix in suffix_lst:\n",
        "        column_name = source + suffix\n",
        "\n",
        "        print(f'__________column_name____________: {column_name}')\n",
        "        columns_to_extract.append(column_name)\n",
        "        print(f'__________columns_to_extract____________: {columns_to_extract}')\n",
        "\n",
        "    df_copy = df_copy.loc[:, columns_to_extract]\n",
        "    print(f'column_extraction function dimensions {df_copy.shape}')\n",
        "    print(f'columns_to_extract: {columns_to_extract}')\n",
        "\n",
        "    return df_copy, columns_to_extract\n",
        "\n",
        "\n",
        "def merge_extracted_df(master_df, indexed_columns_df,\n",
        "        columns_to_extract_lst = None\n",
        "):\n",
        "    '''\n",
        "    Merge the extracted columns dataframe into master key dataframe.\n",
        "    '''\n",
        "    master_df_copy = master_df.copy()\n",
        "\n",
        "    print(f'___________master_df_copy.columns__________{master_df_copy.columns}')\n",
        "\n",
        "    print('before:', master_df_copy.columns)\n",
        "    # print('to merge:', indexed_columns_df.head(2))\n",
        "    indexed_columns_df.to_csv('/content/drive/MyDrive/UChicago/Advanced_ML/indexed_columns_df_error.csv')\n",
        "    print(f'___________left_df_columns___________{indexed_columns_df.columns}')\n",
        "    # if columns_to_extract_lst is not None:\n",
        "    #     master_df_copy = df.drop(columns = columns_to_extract_lst)\n",
        "    master_df_merged = pd.merge(master_df_copy, indexed_columns_df,\n",
        "            on = 'new_index', how = 'left'\n",
        "    )\n",
        "    print(f'___________right_df_columns___________{master_df_copy.columns}')\n",
        "    master_df_merged.to_csv('/content/drive/MyDrive/UChicago/Advanced_ML/indexed_columns_df_error_post.csv')\n",
        "    # print('after:', master_df_copy)\n",
        "\n",
        "    return master_df_merged\n",
        "\n",
        "\n",
        "def index_per_month(master_df, extracted_columns_df, suffix_lst,\n",
        "        columns_to_extract, source\n",
        "):\n",
        "    '''\n",
        "    Assigns an index by whether bill or news article was written in the month in\n",
        "    the extracted columns dataframe.  If it is, assigned a index, otherwise\n",
        "    remains a numpy.nan.  Returns the master dataframe with the new year-month\n",
        "    index.\n",
        "    '''\n",
        "    master_row_len = len(master_df)\n",
        "    print(f'master_row_len = {master_row_len}')\n",
        "    print(f'here_0 columns_to_extract: {columns_to_extract}')\n",
        "    global_index = columns_to_extract[0] # new_index\n",
        "    source_date = columns_to_extract[1]\n",
        "    print(f'__________source_date____________: {source_date}')\n",
        "    months_toindex_lst = columns_to_extract[2:]\n",
        "    extracted_columns_df.loc[:, source_date] = extracted_columns_df.loc[:, source_date].str.slice(0, 7)\n",
        "\n",
        "    for month in months_toindex_lst:\n",
        "        print(f'month {month}')\n",
        "        year_month = month[-7:]\n",
        "        temp_df = extracted_columns_df.copy()\n",
        "        temp_df = temp_df.loc[:, ['new_index', source_date]]\n",
        "        print(f'__________temp_df_columns{temp_df.columns}')\n",
        "        temp_df = temp_df.loc[(temp_df.loc[:, source_date] == year_month), :]\n",
        "        column_index_len = len(temp_df)\n",
        "        print(f'temp_row_len = {column_index_len}')\n",
        "        # print(temp_df.shape, temp_df.columns)\n",
        "        temp_df.loc[:, month] = list(range(len(temp_df)))\n",
        "        temp_df = temp_df.loc[:, ['new_index', month]]\n",
        "        # print(temp_df.shape, temp_df.columns)\n",
        "        print(f\"Attempting to drop: {month}\")\n",
        "        print(f\"Columns before dropping: {master_df.columns}\")\n",
        "        if month in master_df.columns:\n",
        "            print('MONTH IS IN COLUMNS!!!!!')\n",
        "            master_df.drop([month], axis='columns', inplace=True)\n",
        "            print(f\"Columns after dropping: {master_df.columns}\")\n",
        "        else:\n",
        "            print(f\"Column {month} not found in DataFrame.\")\n",
        "        # print(f'__________master_df_columns{master_df.columns}')\n",
        "        master_df = merge_extracted_df(master_df, temp_df, None)\n",
        "\n",
        "    return master_df\n",
        "\n",
        "\n",
        "def all_indexby_month(master_df, sources_lst, bills_suffix_lst, news_suffix_lst,\n",
        "        extract_constants_lst, truncated = False):\n",
        "    '''\n",
        "    Runs the above functions on the data sources of interest, and returns the\n",
        "    master dataframe with the index per month.\n",
        "    '''\n",
        "    df_to_edit = master_df.copy()\n",
        "    new_master_df = master_df.copy()\n",
        "\n",
        "    for source in sources_lst:\n",
        "        print(f'source: {source}')\n",
        "        if source == 'bill':\n",
        "            suffix_lst = bills_suffix_lst\n",
        "        else:\n",
        "            suffix_lst = news_suffix_lst\n",
        "        extracted_columns_df, columns_to_extract = column_extraction(df_to_edit,\n",
        "                source, suffix_lst, ['new_index'], truncated\n",
        "        )\n",
        "        print(f'here_1: {columns_to_extract}')\n",
        "        print(f'head')\n",
        "        print(extracted_columns_df.columns)\n",
        "        new_master_df = index_per_month(new_master_df, extracted_columns_df,\n",
        "                suffix_lst, columns_to_extract, source\n",
        "        )\n",
        "\n",
        "    return new_master_df"
      ],
      "metadata": {
        "id": "qoRF7CvbEiDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_master_index_df = all_indexby_month(master_index_df, sources_lst,\n",
        "        bills_suffix_lst, news_suffix_lst, extract_constants_lst, False)"
      ],
      "metadata": {
        "id": "6SzhFYSpyiOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_indexconlumns_to_int(df):\n",
        "    '''\n",
        "    Takes a dataframe as an input and returns the same dataframe\n",
        "    where all index columns are integers and not floats.\n",
        "    '''\n",
        "    for col in df.columns:\n",
        "        if 'index' in col:\n",
        "            df.loc[:, col] = df.loc[:, col].astype('Int64')\n",
        "    return df"
      ],
      "metadata": {
        "id": "2m4W0KuzS-gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_master_index_df = df_indexconlumns_to_int(new_master_index_df)"
      ],
      "metadata": {
        "id": "rfKnpWRDnUyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_master_index_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/bills_data/v2_master_index_df.csv')"
      ],
      "metadata": {
        "id": "FY2hDf6D_9i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_master_index_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/bills_data/v2_master_index_df.csv')\n",
        "\n",
        "new_master_index_df.to_csv('/content/drive/Shareddrives/PulseofPolicy_data/news_data/v2_master_index_df.csv')"
      ],
      "metadata": {
        "id": "jr6Wd4rMAUrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}