{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXsvd2hC_W2u"
      },
      "outputs": [],
      "source": [
        "! pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "! pip install transformers\n",
        "! pip install sentence_transformers\n",
        "! pip install -U sentence-transformers\n",
        "# ! pip install numpy\n",
        "! pip install torch\n",
        "! pip install torchtext\n",
        "! pip install torchmetrics\n",
        "! pip install pytorch-lightning\n",
        "! pip install time\n",
        "! pip install ipykernel\n",
        "! pip install spacy\n",
        "! pip install \"grpcio>=1.37.0,<2.0\" \"h5py>=3.6.0,<3.7\" \"numpy>=1.22.3,<1.23.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab76DlzRXDUb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "import random\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23pX4NW__lm8"
      },
      "outputs": [],
      "source": [
        "class BERT_Data:\n",
        "    '''\n",
        "    Class that cleans and formats the bills and news datasets for the BERT\n",
        "    model, tokenizes the data, and creates then saves the text embeddings.\n",
        "    '''\n",
        "    def __init__(self, random_seed = 5,\n",
        "                bert_model = 'bert-base-uncased', #'bert-base-uncased' or 'bert-large-uncased'\n",
        "                date_range_begin = None, date_range_end = '2018-04-01',\n",
        "                bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th.csv',\n",
        "                clean_bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_clean.csv',\n",
        "                minimal_clean_bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_clean_minimal.csv',\n",
        "                foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox.csv',\n",
        "                clean_foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox_clean.csv',\n",
        "                minimal_clean_foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox_clean_minimal.csv',\n",
        "                breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart.csv',\n",
        "                clean_breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_clean.csv',\n",
        "                minimal_clean_breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_clean_minimal.csv',\n",
        "                cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn.csv',\n",
        "                clean_cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_clean.csv',\n",
        "                minimal_clean_cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_clean_minimal.csv',\n",
        "                nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt.csv',\n",
        "                clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean.csv',\n",
        "                minimal_clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal.csv',\n",
        "                truncated_minimal_clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal_truncated.csv',\n",
        "                wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date.csv',\n",
        "                clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean.csv',\n",
        "                minimal_clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal.csv',\n",
        "                truncated_minimal_clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal_truncated.csv'\n",
        "    ):\n",
        "        self.device = self.cuda_mps_cpu()\n",
        "        self.random_seed = random_seed\n",
        "        self.random_seed_function()\n",
        "        self.date_range_begin = date_range_begin\n",
        "        self.date_range_end = date_range_end\n",
        "        self.bills_csvpath = bills_csvpath\n",
        "        self.clean_bills_csvpath = clean_bills_csvpath\n",
        "        self.minimal_clean_bills_csvpath = minimal_clean_bills_csvpath\n",
        "        self.df_bills_prepared = pd.read_csv(minimal_clean_bills_csvpath)\n",
        "        # self.df_bills_raw = pd.read_csv(bills_csvpath)\n",
        "        # self.df_bills_clean = self.clean_bills()\n",
        "        # self.df_bills_clean = pd.read_csv(clean_bills_csvpath)\n",
        "        self.foxnews_csvpath = foxnews_csvpath\n",
        "        self.minimal_clean_foxnews_csvpath = minimal_clean_foxnews_csvpath\n",
        "        self.breitbart_csvpath = breitbart_csvpath\n",
        "        self.minimal_clean_breitbart_csvpath = minimal_clean_breitbart_csvpath\n",
        "        self.cnn_csvpath = cnn_csvpath\n",
        "        self.minimal_clean_cnn_csvpath = minimal_clean_cnn_csvpath\n",
        "        self.nytimes_csvpath = nytimes_csvpath\n",
        "        self.minimal_clean_nytimes_csvpath = minimal_clean_nytimes_csvpath\n",
        "        self.truncated_minimal_clean_nytimes_csvpath = truncated_minimal_clean_nytimes_csvpath\n",
        "        self.wapo_csvpath = wapo_csvpath\n",
        "        self.minimal_clean_wapo_csvpath = minimal_clean_wapo_csvpath\n",
        "        self.truncated_minimal_clean_wapo_csvpath = truncated_minimal_clean_wapo_csvpath\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "        self.bert_base = AutoModel.from_pretrained(bert_model)\n",
        "        self.practice_bill = self.df_bills_prepared.head(5).copy()\n",
        "\n",
        "\n",
        "    def clean_bills(self, only_2017 = False, only_2018 = False,\n",
        "                save = True # only_bills = False,\n",
        "    ):\n",
        "        '''\n",
        "        Clean and format the bills dataset\n",
        "        '''\n",
        "        df = pd.read_csv(self.bills_csvpath)\n",
        "        df.loc[:, ['new_index']] = df.index\n",
        "        df.loc[:, 'cleaned_text'] = df.loc[:, 'raw_text'].apply(\n",
        "                self.clean_bill_text, args=()\n",
        "        )\n",
        "        df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'introduced_date'], format='%Y-%m-%d'\n",
        "        )\n",
        "        df.loc[:, ['house_passage_binary']] = df.loc[:, 'house_passage'].fillna(0, inplace=True)\n",
        "\n",
        "        df.loc[:, ['house_passage_binary']] = np.where(\n",
        "                df.loc[:, 'house_passage_binary'] != 0, 1, 0\n",
        "        )\n",
        "\n",
        "        if only_2017:\n",
        "            df = df.loc[(df.loc[:, 'date'] >= '2017-01-01'\n",
        "                    & df.loc[:, 'date'] < '2018-01-01'\n",
        "            ), :]\n",
        "\n",
        "        if only_2018:\n",
        "            df = df.loc[(df.loc[:, 'date'] >= '2018-01-01'\n",
        "                    & df.loc[:, 'date'] < '2019-01-01'\n",
        "            ), :]\n",
        "\n",
        "        df.loc[:, ['cleaned_text']] = df.loc[:, 'cleaned_text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df = df.loc[:, ['bill_id', 'new_index', 'cleaned_text', 'date', 'house_passage_binary', 'bill_type']]\n",
        "\n",
        "        if save:\n",
        "            df.to_csv(self.clean_bills_csvpath, index=False)\n",
        "        # if only_bills:\n",
        "        #     df = df.loc[(df.loc[:, 'bill_type'] == | df.loc[:, 'bill_type'] == ), :]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def dates_clean_news(self, df_whole, Date_version, long_date_version,\n",
        "            start_date = None, end_date = None,\n",
        "            minimal_columns = False\n",
        "            # minimal_columns = ['index', 'date', 'cleaned_text']\n",
        "    ):\n",
        "        '''\n",
        "        Format the date of the news articles to match the date of the bills\n",
        "        '''\n",
        "        if start_date is None:\n",
        "            start_date = self.date_range_begin\n",
        "        if end_date is None:\n",
        "            end_date = self.date_range_end\n",
        "\n",
        "        df = df_whole.copy()\n",
        "        if Date_version:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'Date'], format='%Y-%m-%d'\n",
        "            )\n",
        "        elif long_date_version:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'date'].str[:10], format='%Y-%m-%d'\n",
        "            )\n",
        "        else:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'date'], format='%Y-%m-%d'\n",
        "            )\n",
        "\n",
        "        if start_date is not None:\n",
        "            df = df.loc[df.loc[:, 'date'] >= start_date, :]\n",
        "        if end_date is not None:\n",
        "            df = df.loc[df.loc[:, 'date'] <= end_date, :]\n",
        "\n",
        "        if minimal_columns is not False:\n",
        "            df = df.loc[:, minimal_columns]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def clean_foxnews(self, save = True):\n",
        "        '''\n",
        "        Clean and format the Fox News data\n",
        "        '''\n",
        "        df_fox = pd.read_csv(self.foxnews_csvpath)\n",
        "        df_fox.loc[:, ['new_index']] = df_fox.index\n",
        "        df_fox_dated = self.dates_clean_news(df_fox, Date_version = True,\n",
        "                long_date_version = False\n",
        "        )\n",
        "        df_fox_dated.loc[:, 'cleaned_text'] = df_fox_dated.loc[:, 'article_text'].apply(\n",
        "                self.clean_foxnews_text, args=()\n",
        "        )\n",
        "        df_fox_dated = df_fox_dated.loc[:, ['uuid', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_fox_dated.to_csv(self.minimal_clean_foxnews_csvpath, index=False)\n",
        "\n",
        "        return df_fox_dated\n",
        "\n",
        "\n",
        "    def clean_breitbart(self, save = True):\n",
        "        '''\n",
        "        Clean and format the Breitbart data\n",
        "        '''\n",
        "        df_breitbart = pd.read_csv(self.breitbart_csvpath)\n",
        "        df_breitbart.loc[:, ['new_index']] = df_breitbart.index\n",
        "        df_breitbart_dated = self.dates_clean_news(df_breitbart,\n",
        "                Date_version = True, long_date_version = False\n",
        "        )\n",
        "        df_breitbart_dated.loc[:, 'cleaned_text'] = df_breitbart_dated.loc[:, 'article_text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_breitbart_dated = df_breitbart_dated.loc[:, ['uuid', 'new_index', 'cleaned_text', 'date']]\n",
        "        # df_breitbart_dated.replace('', np.nan, inplace=True)\n",
        "        # df_breitbart_dated.dropna(subset= ['cleaned_text'], inplace=True)\n",
        "\n",
        "        if save:\n",
        "            df_breitbart_dated.to_csv(self.minimal_clean_breitbart_csvpath,\n",
        "                    index=False\n",
        "            )\n",
        "\n",
        "        return df_breitbart_dated\n",
        "\n",
        "\n",
        "    def clean_cnn(self, save = True):\n",
        "        '''\n",
        "        Clean and format the CNN data\n",
        "        '''\n",
        "        df_cnn = pd.read_csv(self.cnn_csvpath)\n",
        "        df_cnn.loc[:, ['new_index']] = df_cnn.index\n",
        "        df_cnn_dated = self.dates_clean_news(df_cnn, Date_version = False,\n",
        "                long_date_version = True\n",
        "        )\n",
        "        df_cnn_dated.loc[:, 'cleaned_text'] = df_cnn_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_cnn_dated = df_cnn_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_cnn_dated.to_csv(self.minimal_clean_cnn_csvpath, index=False)\n",
        "\n",
        "        return df_cnn_dated\n",
        "\n",
        "\n",
        "    def clean_nytimes(self, save = True):\n",
        "        '''\n",
        "        Clean and format the New York Times data\n",
        "        '''\n",
        "        df_nytimes = pd.read_csv(self.nytimes_csvpath)\n",
        "        df_nytimes.loc[:, ['new_index']] = df_nytimes.index\n",
        "        df_nytimes_dated = self.dates_clean_news(df_nytimes,\n",
        "                Date_version = False, long_date_version = True\n",
        "        )\n",
        "        df_nytimes_dated.loc[:, 'cleaned_text'] = df_nytimes_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_nytimes_dated = df_nytimes_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_nytimes_dated.to_csv(self.minimal_clean_nytimes_csvpath,\n",
        "                    index=False\n",
        "            )\n",
        "\n",
        "        return df_nytimes_dated\n",
        "\n",
        "\n",
        "    def clean_wapo(self, save = True):\n",
        "        '''\n",
        "        Clean and format the Washington Post data\n",
        "        '''\n",
        "        df_wapo = pd.read_csv(self.wapo_csvpath)\n",
        "        df_wapo.loc[:, ['new_index']] = df_wapo.index\n",
        "        df_wapo_dated = self.dates_clean_news(df_wapo, Date_version = False,\n",
        "                long_date_version = False\n",
        "        )\n",
        "        df_wapo_dated.loc[:, 'cleaned_text'] = df_wapo_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_wapo_dated = df_wapo_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_wapo_dated.to_csv(self.minimal_clean_wapo_csvpath, index=False)\n",
        "\n",
        "        return df_wapo_dated\n",
        "\n",
        "\n",
        "    def clean_bill_text(self, text, glove=None, cbows=None, lemma=None):\n",
        "        '''\n",
        "        Intakes the text of a single bill and removes formating unique to the\n",
        "        bill text, eliminates html tags, separates sentences,\n",
        "        lower cases, and correctly formats the text\n",
        "        '''\n",
        "        text = text.lower()\n",
        "        text = re.compile('<.*?>').sub('', text)\n",
        "        text = re.compile(r\"[_]\").sub(\" \", text)\n",
        "        text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        text = re.sub(r\"\\d+\\sU\\.S\\.C\\.\\s\\d+[a-z]?(\\(\\d+\\))?( \\([a-z]+\\))?\", \"\",\n",
        "                text\n",
        "        )\n",
        "        text = re.sub(r'(sec\\.\\s+\\d+\\.?)|(section\\s+\\d+\\.)', '', text)\n",
        "        text = re.sub(r'\\(\\d+ u\\.s\\.c\\. \\d+\\([a-z]\\)(\\(\\d+\\))*\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+\\s+u\\.s\\.c\\.\\s+\\d+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+ u\\.s\\.c\\. \\d+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+\\)', '', text)\n",
        "        text = re.sub(r'\\([ivxlcdm]+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\w\\)', '', text)\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'([a-z])\\.--([a-z])', r'\\1. -- \\2', text)\n",
        "        text = text.replace('``', '\"').replace(\"''\", '\"')\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_foxnews_text(self, text):\n",
        "        '''\n",
        "        Intakes a single article and removes formating unique to the Fox News\n",
        "        text, lower cases, and correctly formating the text\n",
        "        '''\n",
        "        text = text.replace(\"Get the latest updates from the 2024 campaign trail, exclusive interviews and more Fox News politics content.SubscribedYou've successfully subscribed to this newsletter!\", \" \")\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"([.!?;:\\\"“”])(?=[^\\s])|”(?=\\w)\", r\"\\1 \", text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_generalnews_text(self, text):\n",
        "        '''\n",
        "        Intakes a single article and removes formatting, lower cases, separates\n",
        "        connected end of sentences, and correctly formating the text\n",
        "        '''\n",
        "        text = re.sub(r\"http\\S+\", \" \", text)\n",
        "        text = re.sub(r\"@\\S+\", \" \", text)\n",
        "        text = re.sub(r'([a-z])\\.”([A-Z])', r'\\1. ” \\2', text)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"([.!?;:\\\"“”])(?=[^\\s])|”(?=\\w)\", r\"\\1 \", text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def long_texts_dim_reduction(self, embedding_tensor,\n",
        "                dim_reduction_strategy\n",
        "    ): # Pooling, max, (potential: PCA)\n",
        "        '''\n",
        "        If tokenized text is untruncated and longer than 510 tokens, adds a\n",
        "        secondary measure to reduce the dimensionality of the embedding\n",
        "        tensor\n",
        "        '''\n",
        "        if dim_reduction_strategy == 'mean':\n",
        "            embedding_tensor = embedding_tensor.mean(dim=0)\n",
        "        elif dim_reduction_strategy == 'max':\n",
        "            embedding_tensor = embedding_tensor.max(dim=0).values\n",
        "\n",
        "        print(embedding_tensor.shape)\n",
        "        return embedding_tensor\n",
        "\n",
        "\n",
        "    def bert_embed_single_row(self, text, index = None, pooling = 'mean',\n",
        "            max_len = 510, attention_mask_onpadding = True,\n",
        "            long_dim_reduction_strategy = 'max',\n",
        "            skip_long_texts = False, truncate_text = False,\n",
        "    ): #512 tokens BERT # max, mean, CLS token embedding\n",
        "        '''\n",
        "        Intakes an entire bill or article, tokenizens the text, embeds the text\n",
        "        using the BERT model, applies pooling, and returns the embedding tensor.\n",
        "        Text may be truncated to max_len tokens.\n",
        "        '''\n",
        "        print(f'Bill/News index: {index}')\n",
        "        if truncate_text:\n",
        "            text_tokenized = self.tokenizer(text, return_tensors='pt',\n",
        "                add_special_tokens=False, truncation=True, max_length = max_len\n",
        "            )\n",
        "        else:\n",
        "            text_tokenized = self.tokenizer(text, return_tensors='pt',\n",
        "                add_special_tokens=False, truncation=False\n",
        "            )\n",
        "        text_len = len(text_tokenized.input_ids[0])\n",
        "        if skip_long_texts:\n",
        "            if text_len > max_len:\n",
        "                return None\n",
        "        # print(text_tokenized.input_ids[0])\n",
        "        text_segment_count = math.ceil(text_len / max_len)\n",
        "        embedding_tensor = torch.tensor([], dtype = torch.float32)\n",
        "        for segment in range(text_segment_count):\n",
        "            start_token = segment * max_len\n",
        "            end_token = min(start_token + max_len, text_len)\n",
        "            # print(f'Segment {segment}, start_token {start_token}, end_token {end_token}')\n",
        "            # print(self.tokenizer.cls_token_id, self.tokenizer.sep_token_id)\n",
        "            segment_ids_tensor = text_tokenized.input_ids[0, start_token:end_token]\n",
        "            padded_token_count = (max_len) - (end_token - start_token)\n",
        "            if padded_token_count == 0:\n",
        "                segment_ids_tensor = torch.cat([\n",
        "                        torch.tensor([self.tokenizer.cls_token_id]),\n",
        "                        segment_ids_tensor,\n",
        "                        torch.tensor([self.tokenizer.sep_token_id])\n",
        "                ]).unsqueeze(0)\n",
        "            else:\n",
        "                padding_tokens_tensor = torch.tensor(\n",
        "                        [self.tokenizer.pad_token_id] * padded_token_count\n",
        "                )\n",
        "                segment_ids_tensor = torch.cat([\n",
        "                        torch.tensor([self.tokenizer.cls_token_id]),\n",
        "                        segment_ids_tensor,\n",
        "                        padding_tokens_tensor,\n",
        "                        torch.tensor([self.tokenizer.sep_token_id])\n",
        "                ]).unsqueeze(0)\n",
        "            if attention_mask_onpadding:\n",
        "                attention_mask = (segment_ids_tensor !=\n",
        "                        self.tokenizer.pad_token_id).to(dtype=torch.int64\n",
        "                )\n",
        "                with torch.no_grad():\n",
        "                    segment_embeddings_tensor = self.bert_base(\n",
        "                            input_ids = segment_ids_tensor,\n",
        "                            attention_mask=attention_mask\n",
        "                    ).last_hidden_state\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    segment_embeddings_tensor = self.bert_base(\n",
        "                            segment_ids_tensor\n",
        "                    ).last_hidden_state\n",
        "\n",
        "            if segment == 0:\n",
        "                total_embeddings_tensor = segment_embeddings_tensor\n",
        "            else:\n",
        "                # print('total_embeddings_tensor: ', total_embeddings_tensor.shape)\n",
        "                # print('segment_embeddings_tensor: ', segment_embeddings_tensor.shape)\n",
        "                total_embeddings_tensor = torch.cat([total_embeddings_tensor,\n",
        "                        segment_embeddings_tensor\n",
        "                ])\n",
        "\n",
        "        if pooling == 'mean':\n",
        "            output_embedding_tensor = total_embeddings_tensor.mean(dim=1)\n",
        "        elif pooling == 'max':\n",
        "            output_embedding_tensor = total_embeddings_tensor.max(dim=1).values\n",
        "        else: # None or CLS implies CLS\n",
        "            output_embedding_tensor = total_embeddings_tensor[:, 0, :]\n",
        "\n",
        "        if not skip_long_texts:\n",
        "            if not truncate_text:\n",
        "                output_embedding_tensor = self.long_texts_dim_reduction(\n",
        "                        output_embedding_tensor, long_dim_reduction_strategy\n",
        "                )\n",
        "\n",
        "        print('output_embedding_tensor: ', output_embedding_tensor.shape)\n",
        "        return output_embedding_tensor\n",
        "\n",
        "\n",
        "    def total_bert_embeddings(self, df, text_column, pooling = 'mean',\n",
        "            max_len = 510, attention_mask_onpadding = True,\n",
        "            long_dim_reduction_strategy = 'max',\n",
        "            skip_long_texts = False, truncate_text = False\n",
        "    ):\n",
        "        '''\n",
        "        Intakes a dataframe, facilitates the tokenization and embedding process\n",
        "        using the BERT model for each row in the dataframe.  Returns the\n",
        "        total embedding tensor.  Text may be truncated to max_len tokens.\n",
        "        '''\n",
        "        embeddings_lst = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            ## Used in testing\n",
        "            # if index == 10:\n",
        "            #     break\n",
        "            embedded_row = self.bert_embed_single_row(row.loc[text_column],\n",
        "                            index, pooling, max_len, attention_mask_onpadding,\n",
        "                            long_dim_reduction_strategy, skip_long_texts,\n",
        "                            truncate_text\n",
        "            )\n",
        "            # If skip_long_texts is True and text after tokenization longer than 512 than text skipped\n",
        "            if embedded_row is not None:\n",
        "                embeddings_lst.append(embedded_row)\n",
        "\n",
        "        embeddings_tensor = torch.stack(embeddings_lst)\n",
        "\n",
        "        return embeddings_tensor\n",
        "\n",
        "\n",
        "    def cuda_mps_cpu(self):\n",
        "        '''\n",
        "        States the computational device in use\n",
        "        '''\n",
        "        if torch.cuda.is_available():  # use GPU if available\n",
        "            print('Using GPU')\n",
        "            return torch.device('cuda')\n",
        "        # https://www.linkedin.com/pulse/how-use-gpu-tensorflow-pytorch-libraries-macbook-pro-m2apple-kashyap/\n",
        "        elif torch.backends.mps.is_available():\n",
        "            print('Using MPS')\n",
        "            return torch.device('mps')\n",
        "        else:\n",
        "            print('Using CPU')\n",
        "            return torch.device('cpu')\n",
        "\n",
        "\n",
        "    def random_seed_function(self):\n",
        "        '''\n",
        "        Sets the random seed for replicability\n",
        "        '''\n",
        "        torch.manual_seed(self.random_seed)\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.manual_seed_all(self.random_seed)\n",
        "\n",
        "        random.seed(self.random_seed)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'/content/drive/MyDrive/30255_data/input_data/115th.json', 'r') as file:\n",
        "    data_base_2 = json.load(file)"
      ],
      "metadata": {
        "id": "ft0mZO1-WN9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data_base_2)"
      ],
      "metadata": {
        "id": "n7B3UrBYWcxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/30255_data/input_data/csv_115th.csv',\n",
        "        index=False\n",
        ")"
      ],
      "metadata": {
        "id": "tYlO8hNHWdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf4_7H-JCmJU",
        "outputId": "8b433d56-e5c8-4c3b-d313-ec74dfbcc3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "data_class = BERT_Data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJdFgEhIqDbV"
      },
      "outputs": [],
      "source": [
        "df_clean_improved = data_class.clean_bills()\n",
        "df_foxnews_clean = data_class.clean_foxnews()\n",
        "df_breitbart_clean = data_class.clean_breitbart()\n",
        "df_cnn_clean = data_class.clean_cnn()\n",
        "df_nytimes_clean = data_class.clean_nytimes()\n",
        "df_wapo_clean = data_class.clean_wapo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSbYEROtz9bJ"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#   Conservative EMBEDDINGs Mean then Mean Pooling   #\n",
        "######################################################\n",
        "df_foxnews_clean = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "foxnews_embeddings_tensor = data_class.total_bert_embeddings(df_foxnews_clean,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/foxnews_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/foxnews_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu2b20i70foT"
      },
      "outputs": [],
      "source": [
        "df_breitbart_clean = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "breitbart_embeddings_tensor = data_class.total_bert_embeddings(df_breitbart_clean,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/breitbart_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u-GJvljH07xR"
      },
      "outputs": [],
      "source": [
        "#############################################\n",
        "#   Left EMBEDDINGS Mean then Mean Pooling  #\n",
        "#############################################\n",
        "cnn_data = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "cnn_embeddings_tensor = data_class.total_bert_embeddings(cnn_data,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/cnn_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py5a8n5R1Fg_"
      },
      "outputs": [],
      "source": [
        "nytimes_data = pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "nytimes_embeddings_tensor = data_class.total_bert_embeddings(nytimes_data,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/nytimes_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nytimes_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytj1dTA51qaQ"
      },
      "outputs": [],
      "source": [
        "wapo_data = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)\n",
        "wapo_embeddings_tensor = data_class.total_bert_embeddings(wapo_data,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/truncated_wapo_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/truncated_wapo_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQzyxcUH1xI2"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "#   Bill EMBEDDINGS Mean then Mean Pooling   #\n",
        "##############################################\n",
        "df_bills_prepared = pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "bills_embeddings_tensor = data_class.total_bert_embeddings(df_bills_prepared,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/bills_embeddings/115th_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_embeddings_mean_mean.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL_jtugYA4Dj"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "#   Conservative EMBEDDINGs Mean then Max Pooling   #\n",
        "#####################################################\n",
        "df_foxnews_clean = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "foxnews_embeddings_tensor = data_class.total_bert_embeddings(\n",
        "        df_foxnews_clean, 'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/foxnews_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/foxnews_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60V3AKhIC2ZM"
      },
      "outputs": [],
      "source": [
        "df_breitbart_clean = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "breitbart_embeddings_tensor = data_class.total_bert_embeddings(\n",
        "        df_breitbart_clean, 'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/breitbart_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCJgTQzk2ewX"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "#   Left EMBEDDINGS Mean then Max Pooling  #\n",
        "############################################\n",
        "cnn_data = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "cnn_embeddings_tensor = data_class.total_bert_embeddings(cnn_data,\n",
        "         'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/cnn_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0oJraTp3G7G"
      },
      "outputs": [],
      "source": [
        "nytimes_data = pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "nytimes_embeddings_tensor = data_class.total_bert_embeddings(nytimes_data,\n",
        "        'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/truncated_nytimes_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/truncated_nytimes_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khwSd2wR3IhV"
      },
      "outputs": [],
      "source": [
        "wapo_data = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)\n",
        "wapo_embeddings_tensor = data_class.total_bert_embeddings(wapo_data,\n",
        "         'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/truncated_wapo_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/truncated_wapo_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M74UDjW3755X"
      },
      "outputs": [],
      "source": [
        "#############################################\n",
        "#   Bill EMBEDDINGS Mean then Max Pooling   #\n",
        "#############################################\n",
        "df_bills_prepared = pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "\n",
        "bills_embeddings_tensor = data_class.total_bert_embeddings(df_bills_prepared,\n",
        "        'cleaned_text'\n",
        ")\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/bills_embeddings/115th_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_embeddings_mean_max.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwP_8isrQqg2"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#   Practice EMBEDDINGs Truncated CLS Only Segment   #\n",
        "######################################################\n",
        "data_class.practice_bill\n",
        "practice_bill_clsembeddings_tensor = data_class.total_bert_embeddings(\n",
        "        data_class.practice_bill,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHIBGOcONgul"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "#   Conservative EMBEDDINGs Truncated CLS Only Segment   #\n",
        "##########################################################\n",
        "df_foxnews_clean = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "foxnews_clsembeddings_tensor = data_class.total_bert_embeddings(df_foxnews_clean,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(foxnews_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/foxnews_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(foxnews_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/foxnews_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFk_bLQPN3zV"
      },
      "outputs": [],
      "source": [
        "df_breitbart_clean = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "breitbart_clsembeddings_tensor = data_class.total_bert_embeddings(df_breitbart_clean,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(breitbart_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/breitbart_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(breitbart_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcz-zr9KN4HH"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "#   Left EMBEDDINGs Truncated CLS Only Segment   #\n",
        "##################################################\n",
        "cnn_data = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "cnn_clsembeddings_tensor = data_class.total_bert_embeddings(cnn_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(cnn_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/cnn_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(cnn_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmeSl-wwN8Vq"
      },
      "outputs": [],
      "source": [
        "nytimes_data = pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "nytimes_clsembeddings_tensor = data_class.total_bert_embeddings(nytimes_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(nytimes_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/nytimes_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(nytimes_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nytimes_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGRGi5cDN9nv"
      },
      "outputs": [],
      "source": [
        "wapo_data = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)\n",
        "wapo_clsembeddings_tensor = data_class.total_bert_embeddings(wapo_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(wapo_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/wapo_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(wapo_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/news_data/wapo_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo2hcLaJN-E4"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "#   Bill EMBEDDINGs Truncated CLS Only Segment   #\n",
        "##################################################\n",
        "bill_data = pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "bill_clsembeddings_tensor = data_class.total_bert_embeddings(bill_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(bill_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/bills_embeddings/bill_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "try:\n",
        "    torch.save(bill_clsembeddings_tensor,\n",
        "            '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/bill_embeddings_cls.pt'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3amolWmdCp_y"
      },
      "outputs": [],
      "source": [
        "a.bert_embed_single_row(a.practice_bill.loc[:, 'cleaned_text'][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}