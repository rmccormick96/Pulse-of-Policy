{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXsvd2hC_W2u"
      },
      "outputs": [],
      "source": [
        "! pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "! pip install transformers\n",
        "! pip install sentence_transformers\n",
        "! pip install -U sentence-transformers\n",
        "# ! pip install numpy\n",
        "! pip install torch\n",
        "! pip install torchtext\n",
        "! pip install torchmetrics\n",
        "! pip install pytorch-lightning\n",
        "! pip install time\n",
        "! pip install ipykernel\n",
        "! pip install spacy\n",
        "! pip install \"grpcio>=1.37.0,<2.0\" \"h5py>=3.6.0,<3.7\" \"numpy>=1.22.3,<1.23.0\"\n",
        "# ! pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "# Basics: dataset, data loaders, Classifier\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "import random\n",
        "import re\n",
        "# import preprocessor as p\n",
        "from sklearn.cluster import KMeans # https://www.youtube.com/watch?v=Kc9gN_gODvQ\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab76DlzRXDUb",
        "outputId": "9421f709-b54c-4da1-a8a9-a5094ee84cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "# Basics: dataset, data loaders, Classifier\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "import random\n",
        "import re\n",
        "# import preprocessor as p\n",
        "from sklearn.cluster import KMeans # https://www.youtube.com/watch?v=Kc9gN_gODvQ\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23pX4NW__lm8"
      },
      "outputs": [],
      "source": [
        "class BERT_Data:\n",
        "    '''\n",
        "    '''\n",
        "    def __init__(self, random_seed = 5,\n",
        "                bert_model = 'bert-base-uncased', #'bert-base-uncased' or 'bert-large-uncased'\n",
        "                date_range_begin = None, date_range_end = '2018-04-01',\n",
        "                bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th.csv',\n",
        "                clean_bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_clean.csv',\n",
        "                minimal_clean_bills_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_clean_minimal.csv',\n",
        "                foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox.csv',\n",
        "                clean_foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox_clean.csv',\n",
        "                minimal_clean_foxnews_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/fox_clean_minimal.csv',\n",
        "                breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart.csv',\n",
        "                clean_breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_clean.csv',\n",
        "                minimal_clean_breitbart_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_clean_minimal.csv',\n",
        "                cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn.csv',\n",
        "                clean_cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_clean.csv',\n",
        "                minimal_clean_cnn_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_clean_minimal.csv',\n",
        "                nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt.csv',\n",
        "                clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean.csv',\n",
        "                minimal_clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal.csv',\n",
        "                truncated_minimal_clean_nytimes_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nyt_clean_minimal_truncated.csv',\n",
        "                wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date.csv',\n",
        "                clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean.csv',\n",
        "                minimal_clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal.csv',\n",
        "                truncated_minimal_clean_wapo_csvpath = '/content/drive/Shareddrives/PulseofPolicy_data/news_data/washington_post_with_date_clean_minimal_truncated.csv'\n",
        "    ):\n",
        "        self.device = self.cuda_mps_cpu()\n",
        "        self.random_seed = random_seed\n",
        "        self.random_seed_function()\n",
        "        self.date_range_begin = date_range_begin\n",
        "        self.date_range_end = date_range_end\n",
        "        self.bills_csvpath = bills_csvpath\n",
        "        self.clean_bills_csvpath = clean_bills_csvpath\n",
        "        self.minimal_clean_bills_csvpath = minimal_clean_bills_csvpath\n",
        "        self.df_bills_prepared = pd.read_csv(minimal_clean_bills_csvpath)\n",
        "        # self.df_bills_raw = pd.read_csv(bills_csvpath)\n",
        "        # self.df_bills_clean = self.clean_bills()\n",
        "        # self.df_bills_clean = pd.read_csv(clean_bills_csvpath)\n",
        "        self.foxnews_csvpath = foxnews_csvpath\n",
        "        self.minimal_clean_foxnews_csvpath = minimal_clean_foxnews_csvpath\n",
        "        self.breitbart_csvpath = breitbart_csvpath\n",
        "        self.minimal_clean_breitbart_csvpath = minimal_clean_breitbart_csvpath\n",
        "        self.cnn_csvpath = cnn_csvpath\n",
        "        self.minimal_clean_cnn_csvpath = minimal_clean_cnn_csvpath\n",
        "        self.nytimes_csvpath = nytimes_csvpath\n",
        "        self.minimal_clean_nytimes_csvpath = minimal_clean_nytimes_csvpath\n",
        "        self.truncated_minimal_clean_nytimes_csvpath = truncated_minimal_clean_nytimes_csvpath\n",
        "        self.wapo_csvpath = wapo_csvpath\n",
        "        self.minimal_clean_wapo_csvpath = minimal_clean_wapo_csvpath\n",
        "        self.truncated_minimal_clean_wapo_csvpath = truncated_minimal_clean_wapo_csvpath\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "        self.bert_base = AutoModel.from_pretrained(bert_model)\n",
        "        self.practice_bill = self.df_bills_prepared.head(5).copy()\n",
        "\n",
        "\n",
        "    def clean_bills(self, only_2017 = False, only_2018 = False,\n",
        "                save = True # only_bills = False,\n",
        "    ):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        df = pd.read_csv(self.bills_csvpath)\n",
        "        df.loc[:, ['new_index']] = df.index\n",
        "        df.loc[:, 'cleaned_text'] = df.loc[:, 'raw_text'].apply(\n",
        "                self.clean_bill_text, args=()\n",
        "        )\n",
        "        df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'introduced_date'], format='%Y-%m-%d'\n",
        "        )\n",
        "        df.loc[:, ['house_passage_binary']] = df.loc[:, 'house_passage'].fillna(0, inplace=True)\n",
        "\n",
        "        df.loc[:, ['house_passage_binary']] = np.where(\n",
        "                df.loc[:, 'house_passage_binary'] != 0, 1, 0\n",
        "        )\n",
        "\n",
        "        if only_2017:\n",
        "            df = df.loc[(df.loc[:, 'date'] >= '2017-01-01'\n",
        "                    & df.loc[:, 'date'] < '2018-01-01'\n",
        "            ), :]\n",
        "\n",
        "        if only_2018:\n",
        "            df = df.loc[(df.loc[:, 'date'] >= '2018-01-01'\n",
        "                    & df.loc[:, 'date'] < '2019-01-01'\n",
        "            ), :]\n",
        "\n",
        "        df.loc[:, ['cleaned_text']] = df.loc[:, 'cleaned_text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df = df.loc[:, ['bill_id', 'new_index', 'cleaned_text', 'date', 'house_passage_binary', 'bill_type']]\n",
        "\n",
        "        if save:\n",
        "            df.to_csv(self.clean_bills_csvpath, index=False)\n",
        "        # if only_bills:\n",
        "        #     df = df.loc[(df.loc[:, 'bill_type'] == | df.loc[:, 'bill_type'] == ), :]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def dates_clean_news(self, df_whole, Date_version, long_date_version,\n",
        "            start_date = None, end_date = None,\n",
        "            minimal_columns = False\n",
        "            # minimal_columns = ['index', 'date', 'cleaned_text']\n",
        "    ):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        if start_date is None:\n",
        "            start_date = self.date_range_begin\n",
        "        if end_date is None:\n",
        "            end_date = self.date_range_end\n",
        "\n",
        "        df = df_whole.copy()\n",
        "        if Date_version:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'Date'], format='%Y-%m-%d'\n",
        "            )\n",
        "        elif long_date_version:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'date'].str[:10], format='%Y-%m-%d'\n",
        "            )\n",
        "        else:\n",
        "            df.loc[:, ['date']] = pd.to_datetime(\n",
        "                    df.loc[:, 'date'], format='%Y-%m-%d'\n",
        "            )\n",
        "\n",
        "        if start_date is not None:\n",
        "            df = df.loc[df.loc[:, 'date'] >= start_date, :]\n",
        "        if end_date is not None:\n",
        "            df = df.loc[df.loc[:, 'date'] <= end_date, :]\n",
        "\n",
        "        if minimal_columns is not False:\n",
        "            df = df.loc[:, minimal_columns]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def clean_foxnews(self, save = True):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        df_fox = pd.read_csv(self.foxnews_csvpath)\n",
        "        df_fox.loc[:, ['new_index']] = df_fox.index\n",
        "        df_fox_dated = self.dates_clean_news(df_fox, Date_version = True,\n",
        "                long_date_version = False\n",
        "        )\n",
        "        df_fox_dated.loc[:, 'cleaned_text'] = df_fox_dated.loc[:, 'article_text'].apply(\n",
        "                self.clean_foxnews_text, args=()\n",
        "        )\n",
        "        df_fox_dated = df_fox_dated.loc[:, ['uuid', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_fox_dated.to_csv(self.minimal_clean_foxnews_csvpath, index=False)\n",
        "\n",
        "        return df_fox_dated\n",
        "\n",
        "\n",
        "    def clean_breitbart(self, save = True):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        df_breitbart = pd.read_csv(self.breitbart_csvpath)\n",
        "        df_breitbart.loc[:, ['new_index']] = df_breitbart.index\n",
        "        df_breitbart_dated = self.dates_clean_news(df_breitbart,\n",
        "                Date_version = True, long_date_version = False\n",
        "        )\n",
        "        df_breitbart_dated.loc[:, 'cleaned_text'] = df_breitbart_dated.loc[:, 'article_text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_breitbart_dated = df_breitbart_dated.loc[:, ['uuid', 'new_index', 'cleaned_text', 'date']]\n",
        "        # df_breitbart_dated.replace('', np.nan, inplace=True)\n",
        "        # df_breitbart_dated.dropna(subset= ['cleaned_text'], inplace=True)\n",
        "\n",
        "        if save:\n",
        "            df_breitbart_dated.to_csv(self.minimal_clean_breitbart_csvpath,\n",
        "                    index=False\n",
        "            )\n",
        "\n",
        "        return df_breitbart_dated\n",
        "\n",
        "\n",
        "    def clean_cnn(self, save = True):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        df_cnn = pd.read_csv(self.cnn_csvpath)\n",
        "        df_cnn.loc[:, ['new_index']] = df_cnn.index\n",
        "        df_cnn_dated = self.dates_clean_news(df_cnn, Date_version = False,\n",
        "                long_date_version = True\n",
        "        )\n",
        "        df_cnn_dated.loc[:, 'cleaned_text'] = df_cnn_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_cnn_dated = df_cnn_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_cnn_dated.to_csv(self.minimal_clean_cnn_csvpath, index=False)\n",
        "\n",
        "        return df_cnn_dated\n",
        "\n",
        "\n",
        "    def clean_nytimes(self, save = True):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        df_nytimes = pd.read_csv(self.nytimes_csvpath)\n",
        "        df_nytimes.loc[:, ['new_index']] = df_nytimes.index\n",
        "        df_nytimes_dated = self.dates_clean_news(df_nytimes,\n",
        "                Date_version = False, long_date_version = True\n",
        "        )\n",
        "        df_nytimes_dated.loc[:, 'cleaned_text'] = df_nytimes_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_nytimes_dated = df_nytimes_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_nytimes_dated.to_csv(self.minimal_clean_nytimes_csvpath,\n",
        "                    index=False\n",
        "            )\n",
        "\n",
        "        return df_nytimes_dated\n",
        "\n",
        "\n",
        "    def clean_wapo(self, save = True):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        df_wapo = pd.read_csv(self.wapo_csvpath)\n",
        "        df_wapo.loc[:, ['new_index']] = df_wapo.index\n",
        "        df_wapo_dated = self.dates_clean_news(df_wapo, Date_version = False,\n",
        "                long_date_version = False\n",
        "        )\n",
        "        df_wapo_dated.loc[:, 'cleaned_text'] = df_wapo_dated.loc[:, 'text'].apply(\n",
        "                self.clean_generalnews_text, args=()\n",
        "        )\n",
        "        df_wapo_dated = df_wapo_dated.loc[:, ['title', 'new_index', 'cleaned_text', 'date']]\n",
        "\n",
        "        if save:\n",
        "            df_wapo_dated.to_csv(self.minimal_clean_wapo_csvpath, index=False)\n",
        "\n",
        "        return df_wapo_dated\n",
        "\n",
        "\n",
        "    def clean_bill_text(self, text, glove=None, cbows=None, lemma=None):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        text = text.lower()\n",
        "        text = re.compile('<.*?>').sub('', text)\n",
        "        text = re.compile(r\"[_]\").sub(\" \", text)\n",
        "        text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        text = re.sub(r\"\\d+\\sU\\.S\\.C\\.\\s\\d+[a-z]?(\\(\\d+\\))?( \\([a-z]+\\))?\", \"\",\n",
        "                text\n",
        "        )\n",
        "        text = re.sub(r'(sec\\.\\s+\\d+\\.?)|(section\\s+\\d+\\.)', '', text)\n",
        "        text = re.sub(r'\\(\\d+ u\\.s\\.c\\. \\d+\\([a-z]\\)(\\(\\d+\\))*\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+\\s+u\\.s\\.c\\.\\s+\\d+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+ u\\.s\\.c\\. \\d+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\d+\\)', '', text)\n",
        "        text = re.sub(r'\\([ivxlcdm]+\\)', '', text)\n",
        "        text = re.sub(r'\\(\\w\\)', '', text)\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'([a-z])\\.--([a-z])', r'\\1. -- \\2', text)\n",
        "        text = text.replace('``', '\"').replace(\"''\", '\"')\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_foxnews_text(self, text):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        text = text.replace(\"Get the latest updates from the 2024 campaign trail, exclusive interviews and more Fox News politics content.SubscribedYou've successfully subscribed to this newsletter!\", \" \")\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"([.!?;:\\\"“”])(?=[^\\s])|”(?=\\w)\", r\"\\1 \", text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_generalnews_text(self, text):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        text = re.sub(r\"http\\S+\", \" \", text)\n",
        "        text = re.sub(r\"@\\S+\", \" \", text)\n",
        "        text = re.sub(r'([a-z])\\.”([A-Z])', r'\\1. ” \\2', text)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"([.!?;:\\\"“”])(?=[^\\s])|”(?=\\w)\", r\"\\1 \", text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def long_texts_dim_reduction(self, embedding_tensor,\n",
        "                dim_reduction_strategy\n",
        "    ): # Pooling, max, (potential: PCA)\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        if dim_reduction_strategy == 'mean':\n",
        "            embedding_tensor = embedding_tensor.mean(dim=0)\n",
        "        elif dim_reduction_strategy == 'max':\n",
        "            embedding_tensor = embedding_tensor.max(dim=0).values\n",
        "\n",
        "        print(embedding_tensor.shape)\n",
        "        return embedding_tensor\n",
        "\n",
        "\n",
        "    def bert_embed_single_row(self, text, index = None, pooling = 'mean',\n",
        "            max_len = 510, attention_mask_onpadding = True,\n",
        "            long_dim_reduction_strategy = 'max',\n",
        "            skip_long_texts = False, truncate_text = False,\n",
        "    ): #512 tokens BERT # max, mean, CLS token embedding\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        print(f'Bill/News index: {index}')\n",
        "        if truncate_text:\n",
        "            text_tokenized = self.tokenizer(text, return_tensors='pt',\n",
        "                add_special_tokens=False, truncation=True, max_length = max_len\n",
        "            )\n",
        "        else:\n",
        "            text_tokenized = self.tokenizer(text, return_tensors='pt',\n",
        "                add_special_tokens=False, truncation=False\n",
        "            )\n",
        "        text_len = len(text_tokenized.input_ids[0])\n",
        "        if skip_long_texts:\n",
        "            if text_len > max_len:\n",
        "                return None\n",
        "        # print(text_tokenized.input_ids[0])\n",
        "        text_segment_count = math.ceil(text_len / max_len)\n",
        "        embedding_tensor = torch.tensor([], dtype = torch.float32)\n",
        "        for segment in range(text_segment_count):\n",
        "            start_token = segment * max_len\n",
        "            end_token = min(start_token + max_len, text_len)\n",
        "            # print(f'Segment {segment}, start_token {start_token}, end_token {end_token}')\n",
        "            # print(self.tokenizer.cls_token_id, self.tokenizer.sep_token_id)\n",
        "            segment_ids_tensor = text_tokenized.input_ids[0, start_token:end_token]\n",
        "            padded_token_count = (max_len) - (end_token - start_token)\n",
        "            if padded_token_count == 0:\n",
        "                segment_ids_tensor = torch.cat([\n",
        "                        torch.tensor([self.tokenizer.cls_token_id]),\n",
        "                        segment_ids_tensor,\n",
        "                        torch.tensor([self.tokenizer.sep_token_id])\n",
        "                ]).unsqueeze(0)\n",
        "            else:\n",
        "                padding_tokens_tensor = torch.tensor(\n",
        "                        [self.tokenizer.pad_token_id] * padded_token_count\n",
        "                )\n",
        "                segment_ids_tensor = torch.cat([\n",
        "                        torch.tensor([self.tokenizer.cls_token_id]),\n",
        "                        segment_ids_tensor,\n",
        "                        padding_tokens_tensor,\n",
        "                        torch.tensor([self.tokenizer.sep_token_id])\n",
        "                ]).unsqueeze(0)\n",
        "            if attention_mask_onpadding:\n",
        "                attention_mask = (segment_ids_tensor !=\n",
        "                        self.tokenizer.pad_token_id).to(dtype=torch.int64\n",
        "                )\n",
        "                with torch.no_grad():\n",
        "                    segment_embeddings_tensor = self.bert_base(\n",
        "                            input_ids = segment_ids_tensor,\n",
        "                            attention_mask=attention_mask\n",
        "                    ).last_hidden_state\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    segment_embeddings_tensor = self.bert_base(\n",
        "                            segment_ids_tensor\n",
        "                    ).last_hidden_state\n",
        "\n",
        "            if segment == 0:\n",
        "                total_embeddings_tensor = segment_embeddings_tensor\n",
        "            else:\n",
        "                # print('total_embeddings_tensor: ', total_embeddings_tensor.shape)\n",
        "                # print('segment_embeddings_tensor: ', segment_embeddings_tensor.shape)\n",
        "                total_embeddings_tensor = torch.cat([total_embeddings_tensor,\n",
        "                        segment_embeddings_tensor\n",
        "                ])\n",
        "\n",
        "        if pooling == 'mean':\n",
        "            output_embedding_tensor = total_embeddings_tensor.mean(dim=1)\n",
        "        elif pooling == 'max':\n",
        "            output_embedding_tensor = total_embeddings_tensor.max(dim=1).values\n",
        "        else: # None or CLS implies CLS\n",
        "            output_embedding_tensor = total_embeddings_tensor[:, 0, :]\n",
        "\n",
        "        if not skip_long_texts:\n",
        "            if not truncate_text:\n",
        "                output_embedding_tensor = self.long_texts_dim_reduction(\n",
        "                        output_embedding_tensor, long_dim_reduction_strategy\n",
        "                )\n",
        "\n",
        "        print('output_embedding_tensor: ', output_embedding_tensor.shape)\n",
        "        return output_embedding_tensor\n",
        "\n",
        "\n",
        "    def total_bert_embeddings(self, df, text_column, pooling = 'mean',\n",
        "            max_len = 510, attention_mask_onpadding = True,\n",
        "            long_dim_reduction_strategy = 'max',\n",
        "            skip_long_texts = False, truncate_text = False\n",
        "    ):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        embeddings_lst = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            ## Used in testing\n",
        "            # if index == 10:\n",
        "            #     break\n",
        "            embedded_row =  self.bert_embed_single_row(row.loc[text_column],\n",
        "                            index, pooling, max_len, attention_mask_onpadding,\n",
        "                            long_dim_reduction_strategy, skip_long_texts,\n",
        "                            truncate_text\n",
        "            )\n",
        "            # If skip_long_texts is True and text after tokenization longer than 512 than text skipped\n",
        "            if embedded_row is not None:\n",
        "                embeddings_lst.append(embedded_row)\n",
        "\n",
        "        embeddings_tensor = torch.stack(embeddings_lst)\n",
        "\n",
        "        return embeddings_tensor\n",
        "\n",
        "\n",
        "    def cuda_mps_cpu(self):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        if torch.cuda.is_available():  # use GPU if available\n",
        "            print('Using GPU')\n",
        "            return torch.device('cuda')\n",
        "        # https://www.linkedin.com/pulse/how-use-gpu-tensorflow-pytorch-libraries-macbook-pro-m2apple-kashyap/\n",
        "        elif torch.backends.mps.is_available():\n",
        "            print('Using MPS')\n",
        "            return torch.device('mps')\n",
        "        else:\n",
        "            print('Using CPU')\n",
        "            return torch.device('cpu')\n",
        "\n",
        "\n",
        "    def random_seed_function(self):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        torch.manual_seed(self.random_seed)\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.manual_seed_all(self.random_seed)\n",
        "\n",
        "        random.seed(self.random_seed)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'/content/drive/MyDrive/30255_data/input_data/115th.json', 'r') as file:\n",
        "    data_base_2 = json.load(file)"
      ],
      "metadata": {
        "id": "ft0mZO1-WN9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data_base_2)"
      ],
      "metadata": {
        "id": "n7B3UrBYWcxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/30255_data/input_data/csv_115th.csv', index=False)"
      ],
      "metadata": {
        "id": "tYlO8hNHWdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf4_7H-JCmJU",
        "outputId": "8b433d56-e5c8-4c3b-d313-ec74dfbcc3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "data_class = BERT_Data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJdFgEhIqDbV"
      },
      "outputs": [],
      "source": [
        "df_clean_improved = data_class.clean_bills()\n",
        "df_foxnews_clean = data_class.clean_foxnews()\n",
        "df_breitbart_clean = data_class.clean_breitbart()\n",
        "df_cnn_clean = data_class.clean_cnn()\n",
        "df_nytimes_clean = data_class.clean_nytimes()\n",
        "df_wapo_clean = data_class.clean_wapo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSbYEROtz9bJ"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#   Conservative EMBEDDINGs Mean then Mean Pooling   #\n",
        "######################################################\n",
        "df_foxnews_clean = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "foxnews_embeddings_tensor = data_class.total_bert_embeddings(df_foxnews_clean,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/foxnews_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/foxnews_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu2b20i70foT"
      },
      "outputs": [],
      "source": [
        "df_breitbart_clean = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "breitbart_embeddings_tensor = data_class.total_bert_embeddings(df_breitbart_clean,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/breitbart_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u-GJvljH07xR"
      },
      "outputs": [],
      "source": [
        "#############################################\n",
        "#   Left EMBEDDINGS Mean then Mean Pooling  #\n",
        "#############################################\n",
        "cnn_data = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "cnn_embeddings_tensor = data_class.total_bert_embeddings(cnn_data,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/cnn_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py5a8n5R1Fg_"
      },
      "outputs": [],
      "source": [
        "nytimes_data = pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "nytimes_embeddings_tensor = data_class.total_bert_embeddings(nytimes_data,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/nytimes_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nytimes_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytj1dTA51qaQ"
      },
      "outputs": [],
      "source": [
        "wapo_data = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)\n",
        "wapo_embeddings_tensor = data_class.total_bert_embeddings(wapo_data,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/truncated_wapo_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/truncated_wapo_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQzyxcUH1xI2"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "#   Bill EMBEDDINGS Mean then Mean Pooling   #\n",
        "##############################################\n",
        "df_bills_prepared = pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "bills_embeddings_tensor = data_class.total_bert_embeddings(df_bills_prepared,\n",
        "        'cleaned_text', pooling = 'mean', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = 'mean',\n",
        "        skip_long_texts = False, truncate_text = False\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/bills_embeddings/115th_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_embeddings_mean_mean.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL_jtugYA4Dj"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "#   Conservative EMBEDDINGs Mean then Max Pooling   #\n",
        "#####################################################\n",
        "df_foxnews_clean = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "foxnews_embeddings_tensor = data_class.total_bert_embeddings(\n",
        "        df_foxnews_clean, 'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/foxnews_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(foxnews_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/foxnews_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60V3AKhIC2ZM"
      },
      "outputs": [],
      "source": [
        "df_breitbart_clean = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "breitbart_embeddings_tensor = data_class.total_bert_embeddings(\n",
        "        df_breitbart_clean, 'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/breitbart_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(breitbart_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCJgTQzk2ewX"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "#   Left EMBEDDINGS Mean then Max Pooling  #\n",
        "############################################\n",
        "cnn_data = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "cnn_embeddings_tensor = data_class.total_bert_embeddings(cnn_data,\n",
        "         'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/cnn_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(cnn_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0oJraTp3G7G"
      },
      "outputs": [],
      "source": [
        "nytimes_data = pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "nytimes_embeddings_tensor = data_class.total_bert_embeddings(nytimes_data,\n",
        "        'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/truncated_nytimes_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(nytimes_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/truncated_nytimes_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khwSd2wR3IhV"
      },
      "outputs": [],
      "source": [
        "wapo_data = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)\n",
        "wapo_embeddings_tensor = data_class.total_bert_embeddings(wapo_data,\n",
        "         'cleaned_text'\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/truncated_wapo_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(wapo_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/truncated_wapo_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M74UDjW3755X"
      },
      "outputs": [],
      "source": [
        "#############################################\n",
        "#   Bill EMBEDDINGS Mean then Max Pooling   #\n",
        "#############################################\n",
        "df_bills_prepared = pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "\n",
        "bills_embeddings_tensor = data_class.total_bert_embeddings(df_bills_prepared,\n",
        "        'cleaned_text'\n",
        ")\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/bills_embeddings/115th_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(bills_embeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/115th_embeddings_mean_max.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwP_8isrQqg2"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#   Practice EMBEDDINGs Truncated CLS Only Segment   #\n",
        "######################################################\n",
        "data_class.practice_bill\n",
        "practice_bill_clsembeddings_tensor = data_class.total_bert_embeddings(\n",
        "        data_class.practice_bill,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHIBGOcONgul"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "#   Conservative EMBEDDINGs Truncated CLS Only Segment   #\n",
        "##########################################################\n",
        "df_foxnews_clean = pd.read_csv(data_class.minimal_clean_foxnews_csvpath)\n",
        "foxnews_clsembeddings_tensor = data_class.total_bert_embeddings(df_foxnews_clean,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(foxnews_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/foxnews_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(foxnews_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/foxnews_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFk_bLQPN3zV"
      },
      "outputs": [],
      "source": [
        "df_breitbart_clean = pd.read_csv(data_class.minimal_clean_breitbart_csvpath)\n",
        "breitbart_clsembeddings_tensor = data_class.total_bert_embeddings(df_breitbart_clean,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(breitbart_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/breitbart_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(breitbart_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/breitbart_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcz-zr9KN4HH"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "#   Left EMBEDDINGs Truncated CLS Only Segment   #\n",
        "##################################################\n",
        "cnn_data = pd.read_csv(data_class.minimal_clean_cnn_csvpath)\n",
        "cnn_clsembeddings_tensor = data_class.total_bert_embeddings(cnn_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(cnn_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/cnn_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(cnn_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/cnn_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmeSl-wwN8Vq"
      },
      "outputs": [],
      "source": [
        "nytimes_data = pd.read_csv(data_class.truncated_minimal_clean_nytimes_csvpath)\n",
        "nytimes_clsembeddings_tensor = data_class.total_bert_embeddings(nytimes_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(nytimes_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/nytimes_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(nytimes_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/nytimes_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGRGi5cDN9nv"
      },
      "outputs": [],
      "source": [
        "wapo_data = pd.read_csv(data_class.truncated_minimal_clean_wapo_csvpath)\n",
        "wapo_clsembeddings_tensor = data_class.total_bert_embeddings(wapo_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(wapo_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_embeddings/wapo_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    torch.save(wapo_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/news_data/wapo_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo2hcLaJN-E4"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "#   Bill EMBEDDINGs Truncated CLS Only Segment   #\n",
        "##################################################\n",
        "bill_data = pd.read_csv(data_class.minimal_clean_bills_csvpath)\n",
        "bill_clsembeddings_tensor = data_class.total_bert_embeddings(bill_data,\n",
        "        'cleaned_text', pooling = 'CLS', max_len = 510,\n",
        "        attention_mask_onpadding = True, long_dim_reduction_strategy = None,\n",
        "        skip_long_texts = False, truncate_text = True\n",
        ")\n",
        "\n",
        "try:\n",
        "    torch.save(bill_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/bills_embeddings/bill_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "try:\n",
        "    torch.save(bill_clsembeddings_tensor, '/content/drive/Shareddrives/PulseofPolicy_data/bills_data/bill_embeddings_cls.pt')\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3amolWmdCp_y",
        "outputId": "20be182e-f0a2-4a3b-a243-d6989e639ed9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5924 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segment 0, start_token 0, end_token 510\n",
            "101 102\n",
            "tensor([[  101, 10630,  2705,  3519, 14134,  5219,  1044,  1012,  1054,  1012,\n",
            "         25833,  2487,  2000, 19933,  8910,  1997,  2375,  8800,  2000, 13141,\n",
            "         25141, 24007,  1010,  1998,  2005,  2060,  5682,  1012,  1999,  1996,\n",
            "          2160,  1997,  4505,  2254,  1016,  1010, 10476,  2720,  1012, 21513,\n",
            "          3107,  1996,  2206,  3021,  1025,  2029,  2001,  3615,  2000,  1996,\n",
            "          2837,  2006,  1996, 14814,  1010,  1998,  1999,  2804,  2000,  1996,\n",
            "          9528,  2006,  3097,  3821,  1010,  1998,  3971,  1998,  2965,  1010,\n",
            "          2005,  1037,  2558,  2000,  2022,  3525,  4340,  2011,  1996,  5882,\n",
            "          1010,  1999,  2169,  2553,  2005,  9584,  1997,  2107,  8910,  2004,\n",
            "          2991,  2306,  1996,  7360,  1997,  1996,  2837,  4986,  1037,  3021,\n",
            "          2000, 19933,  8910,  1997,  2375,  8800,  2000, 13141, 25141, 24007,\n",
            "          1010,  1998,  2005,  2060,  5682,  1012,  2022,  2009, 11955,  2011,\n",
            "          1996,  4001,  1998,  2160,  1997,  4505,  1997,  1996,  2142,  2163,\n",
            "          1997,  2637,  1999,  3519,  9240,  1010,  2460,  2516,  1025,  2795,\n",
            "          1997,  8417,  1012,  2460,  2516,  1012,  1011,  1011,  2023,  2552,\n",
            "          2089,  2022,  6563,  2004,  1996,  1000, 16003, 13141, 25141, 24007,\n",
            "          2552,  1000,  1012,  2795,  1997,  8417,  1012,  1011,  1011,  1996,\n",
            "          2795,  1997,  8417,  2005,  2023,  2552,  2003,  2004,  4076,  1024,\n",
            "          2460,  2516,  1025,  2795,  1997,  8417,  1012,  2394,  2653,  1998,\n",
            "          2147, 10296,  2731,  2005,  4844, 13141, 17362,  1012, 26410,  4568,\n",
            "          6319,  3570,  1997,  8711,  1998, 12114,  4379, 11386,  1012, 10651,\n",
            "          1997,  7684,  1998, 11073,  8624,  1012, 12016,  1997, 13141,  2565,\n",
            "          6736,  1012,  2553,  2968,  1012,  3623,  1999,  5356, 10504,  1012,\n",
            "         13141,  8346,  8624,  1012,  9844,  3946,  2565,  4935,  1012,  4968,\n",
            "          5057, 13141, 25141, 24007,  4636,  1012, 27024,  3036,  3318,  6666,\n",
            "          1012,  2437,  2569, 11560, 11799,  3570,  3841, 12879, 24108,  5134,\n",
            "          1998, 14477, 21408,  8737,  7088,  2098,  2336,  4379,  1000,  1057,\n",
            "          1000,  9425,  3860,  7792,  2005,  6469,  2729,  1998, 13141,  6666,\n",
            "          1012,  2394,  2653,  1998,  2147, 10296,  2731,  2005,  4844, 13141,\n",
            "         17362,  1012,  1999,  2236,  1012,  1011,  1011,  1996,  3187,  1997,\n",
            "          2110,  4618,  5323,  6931, 13141,  2731,  3454,  2000,  3749,  2394,\n",
            "          2004,  1037,  2117,  2653,  1998,  2147, 10296,  2731,  7047,  2005,\n",
            "          8711,  2040,  2031,  2042,  4844,  2005,  9634,  2000,  1996,  2142,\n",
            "          2163,  2077,  2037,  6712,  2005,  1996,  2142,  2163,  1012,  2640,\n",
            "          1998,  7375,  1012,  1011,  1011,  1999, 12697,  1998, 14972,  1996,\n",
            "          3454,  3615,  2000,  1999,  4942, 29015,  1010,  1996,  3187,  4618,\n",
            "         23363,  2007,  2030,  2224,  1011,  1011,  2512,  3995, 23062, 26901,\n",
            "          2030,  2248,  4411,  2007,  3622,  7208,  2000,  1996,  2142,  2163,\n",
            "         13141, 25141, 24007,  2565,  1025,  1998,  2512,  3995, 23062, 26901,\n",
            "          2030,  2248,  4411,  2007,  6413, 11532,  1999,  4975,  8882,  1998,\n",
            "          4252,  2394,  2004,  1037,  2117,  2653,  1012,  4254,  2006,  6364,\n",
            "          2335,  1012,  1011,  1011,  1996,  3187,  4618,  5676,  2008,  2107,\n",
            "          2731,  3454,  2024,  9975, 11887,  1010,  5258,  2306, 12711,  6364,\n",
            "          2335,  1998,  2079,  2025,  8536,  2030,  4652,  1996,  6712,  2005,\n",
            "          1996,  2142,  2163,  1997,  8711,  2040,  2031,  2042,  4844,  2005,\n",
            "          9634,  2000,  1996,  2142,  2163,  1012, 17060,  2005,  7375,  1012,\n",
            "          1011,  1011,  3988,  7375,  1012,  1011,  1011,  2025,  2101,  2084,\n",
            "          1015,  2095,  2044,  1996,  3058,  1997,  1996, 26465,  1997,  2023,\n",
            "          2552,  1010,  1996,  3187,  4618,  5676,  2008,  2107,  2731,  3454,\n",
            "          2024,  3929,  1998, 14678,  6515,  1999,  2012,  2560,  1017, 13141,\n",
            "          6364,  4655,  1012,  3176,  7375,  1012,  1011,  1011,  2025,  2101,\n",
            "          2084,   102]])\n",
            "Segment 1, start_token 510, end_token 1020\n",
            "101 102\n",
            "tensor([[  101,  1016,  2086,  2044,  1996,  3058,  1997,  1996, 26465,  1997,\n",
            "          2023,  2552,  1010,  1996,  3187,  4618,  2025,  8757,  1996,  6413,\n",
            "          7740,  9528,  2008,  2107,  2731,  3454,  2024,  3929,  1998, 10862,\n",
            "          6515,  1999,  1019, 13141,  6364,  4655,  1012, 17377,  3189,  1012,\n",
            "          1011,  1011,  2025,  2101,  2084,  1018,  2086,  2044,  1996,  3058,\n",
            "          1997,  1996, 26465,  1997,  2023,  2552,  1010,  1996,  4012, 13876,\n",
            "         26611,  2236,  1997,  1996,  2142,  2163,  4618,  6204,  1037,  2817,\n",
            "          2006,  1996,  7375,  1997,  2023,  2930,  1010,  2164,  2019,  7667,\n",
            "          1997,  1996,  3737,  1997,  2394,  2004,  1037,  2117,  2653,  8882,\n",
            "          1998,  7899,  1010,  1996,  6666,  1997,  1996,  2147, 10296,  1998,\n",
            "          2394,  2004,  1037,  2117,  2653,  2731,  2565,  2000,  8711,  1010,\n",
            "          1998, 11433,  2006,  3251,  2107,  3454,  2323,  2022,  2506,  1010,\n",
            "          5041,  6675,  1010,  2030,  6310,  1010,  1998,  4618, 12040,  2000,\n",
            "          1996,  6413,  7740,  9528,  1037,  3189,  2006,  1996,  9556,  1997,\n",
            "          2107,  2817,  1012,  3627,  1997,  2810,  1012,  1011,  1011,  2498,\n",
            "          1999,  2023,  2930,  4618,  2022,  9530,  3367, 28551,  2000,  5478,\n",
            "          2008,  1037, 13141,  5589,  1999,  2107,  1037,  2731,  2565,  2004,\n",
            "          1037,  3653,  8663, 20562,  2005,  1996,  9634,  2000,  1996,  2142,\n",
            "          2163,  1997,  2107, 13141,  1012, 26410,  4568,  6319,  3570,  1997,\n",
            "          8711,  1998, 12114,  4379, 11386,  1012,  9634,  1997,  5057,  3663,\n",
            "          8711,  1012,  1011,  1011,  2930, 19843,  1997,  1996,  7521,  1998,\n",
            "         10662,  2552,  2003, 13266,  1011,  1011,  1999, 20423,  1011,  1011,\n",
            "          2011,  8478,  1000,  4905,  2236,  1000,  1996,  2034,  2051,  2009,\n",
            "          3544,  1998, 19274,  2075,  1000,  3187,  1997, 10759,  3036,  1000,\n",
            "          1025,  2011,  8478,  1000,  4905,  2236,  1000,  2169,  3176,  2173,\n",
            "          2009,  3544,  1998, 19274,  2075,  1000,  3187,  1000,  1025,  1998,\n",
            "          2011,  8478,  1000,  1006,  3272,  2004,  4728,  3024,  2104, 20423,\n",
            "          1007,  2004,  2019, 11560,  2104,  2023,  2552,  1012,  1000,  1998,\n",
            "         19274,  2075,  1000,  1006,  3272,  2004,  3024,  2104,  4942, 29015,\n",
            "          2015,  1998,  1997,  2930, 19348,  1007,  2004,  2019, 11560,  2104,\n",
            "          2023,  2552,  1012, 26206,  2151, 15973, 12546,  9675,  1999,  2023,\n",
            "          2552,  1010,  2151,  7344,  4914,  2104,  2023, 20423,  4618,  2022,\n",
            "          5240,  2004, 26410,  2135,  4914,  2000,  1996,  2142,  2163,  2005,\n",
            "          4568,  5039,  2004,  1997,  1996,  3058,  1997,  2107,  7344,  1005,\n",
            "          1055,  9634,  2000,  1996,  2142,  2163,  1012,  1000,  1025,  1999,\n",
            "         20423,  1011,  1011,  2011,  8478,  1000,  1006,  3272,  2004,  4728,\n",
            "          3024,  2104, 20423,  1007,  1000,  1998, 19274,  2075,  1000,  1006,\n",
            "          3272,  2004,  3024,  2104,  4942, 29015,  2015,  1998,  1997,  2930,\n",
            "         19348,  1007,  1000,  1025,  1998,  2011,  8478,  1996,  2197,  6251,\n",
            "          1998, 19274,  2075,  1996,  2206,  1024,  1000,  2019,  7344,  4914,\n",
            "          2000,  1996,  2142,  2163,  2004,  1037, 13141,  2089,  9964,  2005,\n",
            "          2010,  2030,  2014, 18591,  2030,  2775,  2000,  3582,  2000,  3693,\n",
            "          2032,  2030,  2014,  1999,  1996,  2142,  2163,  2012,  2151,  2051,\n",
            "          2044,  2107,  7344,  1005,  1055,  9634,  1010, 26206,  2010,  2030,\n",
            "          2014,  3949,  2004,  1037, 26410,  4568,  6319,  2004,  1997,  1996,\n",
            "          3058,  1997,  2010,  2030,  2014,  9634,  2000,  1996,  2142,  2163,\n",
            "          1012,  1000,  1025,  2011,  8478, 20423,  1025,  2011, 25136,  5844,\n",
            "         20423,  2004, 20423,  1025,  1998,  1999, 20423,  1010,  2004, 11836,\n",
            "          1011,  1011,  2011,  8478,  1000,  4905,  2236,  1000,  1996,  2034,\n",
            "          2173,  2009,  3544,  1998, 19274,  2075,  1000,  3187,  1997, 10759,\n",
            "          3036,  1000,  1025,  1998,  2011,  8478,  1000,  4905,  2236,  1000,\n",
            "          2169,   102]])\n",
            "Segment 2, start_token 1020, end_token 1530\n",
            "101 102\n",
            "tensor([[  101,  3176,  2173,  2009,  3544,  1998, 19274,  2075,  1000,  3187,\n",
            "          1000,  1012,  3949,  1997, 18591,  1998,  2336,  1012,  1011,  1011,\n",
            "          2930, 18512,  1997,  2107,  2552,  2003, 13266,  1011,  1011,  2011,\n",
            "         25136,  5844,  4942, 28689, 14413,  2004,  4942, 28689, 14413,  1025,\n",
            "          1998,  2011, 19274,  2075,  2044,  4942, 28689, 14413,  1996,  2206,\n",
            "          1024,  1000,  9964,  1012,  1011,  1011,  2019,  7344,  4379, 11386,\n",
            "          2104,  2023,  4942, 29015,  2089,  9964,  2005,  1996,  2168,  3570,\n",
            "          2000,  2022, 15186,  2006,  2010,  2030,  2014, 18591,  2030,  2775,\n",
            "          2012,  2151,  2051,  2044,  2107,  7344,  2003,  4379, 11386,  3251,\n",
            "          2030,  2025,  2107,  7344,  2038,  4162,  2005,  1010,  2030,  2042,\n",
            "          4379,  1010, 19037,  2000,  4568,  6319,  3570,  2104,  1000,  4568,\n",
            "          6319,  3570,  1012,  1011,  1011, 26206,  2151, 15973, 12546,  9675,\n",
            "          1999,  2023,  2552,  1010,  1037, 18591,  2030,  2775,  4914,  2000,\n",
            "          1996,  2142,  2163,  2004,  2019,  2004, 12844,  2063,  2206,  2000,\n",
            "          3693,  1037, 18591,  2030,  6687,  3130,  4379, 11386,  4618,  2022,\n",
            "          5240,  2004, 26410,  2135,  4914,  2000,  1996,  2142,  2163,  2005,\n",
            "          4568,  5039,  2004,  1997,  1996,  3058,  1997,  2107, 18591,  1005,\n",
            "          1055,  2030,  2775,  1005,  1055,  9634,  2000,  1996,  2142,  2163,\n",
            "          1012,  1000,  4646,  2005, 19037,  1997,  3570,  1012,  1011,  1011,\n",
            "          1037, 18591,  2030,  2775,  2040,  2001,  2025,  4914,  2000,  1996,\n",
            "          2142,  2163, 27081,  2000,  1037,  3946,  1997, 11386,  1010,  2021,\n",
            "          2040,  2001,  4379, 11386,  2104,  2023,  4942, 28689, 14413,  2044,\n",
            "          2010,  2030,  2014,  5508,  2004,  1996, 18591,  2030,  2775,  1997,\n",
            "          2019,  7344,  4379, 11386,  2104,  2930, 18512,  1010,  2089,  6611,\n",
            "          2005, 19037,  1997,  3570,  2000,  2008,  1997, 26410,  4568,  6319,\n",
            "          2104,  2930, 19348,  2012,  2151,  2051,  2044,  2108,  4379, 11386,\n",
            "          1012,  1000,  1012,  8711,  1012,  1011,  1011,  2930, 19348,  1997,\n",
            "          2107,  2552,  2003, 13266,  2000,  3191,  2004,  4076,  1024,  1000,\n",
            "          3949,  1997, 12114,  4914,  2004,  8711,  1998, 12114,  4379, 11386,\n",
            "          1012,  1000,  1999,  2236,  1012,  1011,  1011,  1000,  3949,  1997,\n",
            "          8711,  1012,  1011,  1011, 26206,  2151, 15973, 12546,  9675,  1999,\n",
            "          2023,  2552,  1010,  2151,  7344,  2040,  2038,  2042,  4914,  2000,\n",
            "          1996,  2142,  2163,  2104,  2930, 19843,  4618,  2022,  5240,  2004,\n",
            "         26410,  2135,  4914,  2000,  1996,  2142,  2163,  2005,  4568,  5039,\n",
            "          2004,  1997,  1996,  3058,  1997,  2107,  9634,  1012,  1000,  3949,\n",
            "          1997, 18591,  1998,  2336,  1012,  1011,  1011, 26206,  2151, 15973,\n",
            "         12546,  9675,  1999,  2023,  2552,  1010,  2151,  7344,  4914,  2000,\n",
            "          1996,  2142,  2163,  2104,  2930, 18512,  2004,  1996, 18591,  2030,\n",
            "          2775,  1997,  2019,  7344,  4379, 11386,  2104,  2930, 18512,  4618,\n",
            "          2022,  5240,  2004, 26410,  2135,  4914,  2000,  1996,  2142,  2163,\n",
            "          2005,  4568,  5039,  2004,  1997,  1996,  3058,  1997,  2107,  9634,\n",
            "          1012,  1000, 19037,  1997,  3570,  1012,  1011,  1011,  1996,  3187,\n",
            "          1997, 10759,  3036,  2030,  1996,  4905,  2236,  1010,  1999,  1996,\n",
            "         19258,  1997,  1996,  3187,  2030,  1996,  4905,  2236,  1010,  1998,\n",
            "          2104,  2107,  7040,  2004,  1996,  3187,  2030,  1996,  4905,  2236,\n",
            "          2089,  3653, 29234,  1010,  2089, 14171,  1010,  2000,  1996,  3570,\n",
            "          1997,  2019,  7344, 26410,  2135,  4914,  2000,  1996,  2142,  2163,\n",
            "          2005,  4568,  5039,  1010,  1996,  3570,  1997,  2151,  7344,  2040,\n",
            "          1010,  2096,  1999,  1996,  2142,  2163,  1011,  1011,  1000,  2003,\n",
            "          4379,  1011,  1011,  1000, 11386,  2104,  2930, 18512,  1006,  2004,\n",
            "          1037,  4054,  7344,  2030,  2004,  1996, 18591,  2030,  2775,  1997,\n",
            "          2019,   102]])\n",
            "Segment 3, start_token 1530, end_token 2040\n",
            "101 102\n",
            "tensor([[  101,  7344,  4379, 11386,  1007,  1025,  2030,  1000, 13141,  3570,\n",
            "          2104,  2930, 19843,  2004,  1996, 18591,  2030,  2775,  1997,  1037,\n",
            "         13141,  1025,  1000, 12033,  2005,  2107, 19037,  1997,  3570,  2012,\n",
            "          2151,  2051,  2044,  2108,  4379, 11386,  2030, 13141,  3570,  1025,\n",
            "          1000,  2003,  2025,  7933, 25141, 14782,  1999,  2151,  3097,  2406,\n",
            "          1025,  2030,  1000,  2003,  4748, 26770,  1006,  3272,  2004,  4728,\n",
            "          3024,  2104,  4942, 29015,  2015,  1998,  1007,  2004,  2019, 11560,\n",
            "          2104,  2023,  2552,  2012,  1996,  2051,  1997,  7749,  2005, 19037,\n",
            "          1997,  2107,  7344,  1012,  1000,  2501,  1012,  1011,  1011,  2588,\n",
            "          6226,  1997,  2019,  4646,  2104,  2023,  4942, 29015,  1010,  1996,\n",
            "          3187,  1997, 10759,  3036,  2030,  1996,  4905,  2236,  4618,  5323,\n",
            "          1037,  2501,  1997,  1996,  7344,  1005,  1055,  9634,  2005, 26410,\n",
            "          4568,  5039,  2004,  1997,  1996,  3058,  2107,  7344,  2001,  4379,\n",
            "         11386,  2030, 13141,  3570,  1012,  1000,  6254, 26354, 26620,  1012,\n",
            "          1011,  1011,  2019,  7344,  2040,  2038,  2042,  4914,  2000,  1996,\n",
            "          2142,  2163,  2104,  2930, 19843,  2030, 18512,  2030,  2040, 14171,\n",
            "          2015,  2000,  1996,  3570,  1997,  1037, 26410,  4568,  6319,  2004,\n",
            "          1037, 13141,  2030,  2004, 12844,  2063,  2104,  2023,  2930,  4618,\n",
            "          2022,  3843, 12653,  8131,  2008,  2107,  7344,  2003,  1037, 26410,\n",
            "          4568,  6319, 27081,  2000,  1037,  3946,  1997, 13141,  2030, 11386,\n",
            "          3570,  1012,  1000, 27118,  9397, 19341,  8553,  1997,  3056, 27118,\n",
            "         22117, 14643, 13464,  5286,  2000,  8711,  1010, 12114,  4379, 11386,\n",
            "          1010,  1998,  2107, 12114,  6224, 19037,  1997,  3570,  2000, 26410,\n",
            "          4568,  6319,  1012,  1011,  1011, 20423,  2015,  1010,  1010,  1998,\n",
            "          1997,  2930, 18164,  4618,  2025,  6611,  2000,  1011,  1011,  1000,\n",
            "          2151, 13141,  2104,  2930, 19843,  1025,  1000,  2151,  7344,  4379,\n",
            "         11386,  2104,  2930, 18512,  1025,  2030,  1000,  2151,  7344,  6224,\n",
            "          9634,  2004,  1037, 26410,  4568,  6319, 27081,  2000,  1037,  3946,\n",
            "          1997, 13141,  2030, 11386,  3570,  1012,  1000, 23701,  6299,  1997,\n",
            "         27118, 22117, 14643, 13464,  2030,  2139,  6442,  8010,  2005,  8711,\n",
            "          1010, 12114,  4379, 11386,  1010,  1998,  2107, 12114,  6224, 19037,\n",
            "          1997,  3570,  2000, 26410,  4568,  6319,  1012,  1011,  1011,  1000,\n",
            "          1999,  2236,  1012,  1011,  1011,  3272,  2004,  3024,  1999, 20423,\n",
            "          1010,  1996,  3187,  1997, 10759,  3036,  2030,  1996,  4905,  2236,\n",
            "          2089, 23701,  3726,  2151,  2598,  1997, 27118, 22117, 14643, 13464,\n",
            "          2104,  2930, 18164,  2030,  2151,  2598,  1997,  2139,  6442,  8010,\n",
            "          2104,  2930, 23297,  2005,  1037, 13141,  4914,  2104,  2930, 19843,\n",
            "          1010,  2019,  7344,  4379, 11386,  2104,  2930, 18512,  1010,  2030,\n",
            "          2019,  7344,  6224,  9634,  2004,  1037, 26410,  4568,  6319, 27081,\n",
            "          2000,  1037,  3946,  1997, 13141,  2030, 11386,  3570,  2065,  1996,\n",
            "          3187,  2030,  1996,  4905,  2236, 16463,  2008,  2107, 23701,  6299,\n",
            "          2003, 15123,  2011, 11470,  5682,  1010,  2000,  5676,  2155,  8499,\n",
            "          1010,  2030,  2003,  4728,  1999,  1996,  2270,  3037,  1012,  1000,\n",
            "          1999, 20806,  5856,  8553,  1012,  1011,  1011,  1037, 13141,  2104,\n",
            "          2930, 19843,  1010,  2019,  7344,  4379, 11386,  2104,  2930, 18512,\n",
            "          1010,  2030,  2019,  7344,  6224,  9634,  2004,  1037, 26410,  4568,\n",
            "          6319, 27081,  2000,  1037,  3946,  1997, 13141,  2030, 11386,  3570,\n",
            "          4618,  2022, 22023,  2005,  1037, 23701,  6299,  2104, 20423,  2065,\n",
            "          2009,  2038,  2042,  2511,  2008,  1996,  7344,  2003,  1011,  1011,\n",
            "          1000, 27118, 22117, 14643,  7028,  2104,  2930, 18164,  2030,  4942,\n",
            "         28689, 14413,  1010,  1010,  1010,  2030,  1997,  2930, 18164,  1025,\n",
            "          1000,   102]])\n",
            "Segment 4, start_token 2040, end_token 2550\n",
            "101 102\n",
            "tensor([[  101,  2139,  6442,  3085,  2104,  2930, 23297,  2005,  2019, 10048,\n",
            "          2649,  1999,  2930,  7886,  1025,  2030,  1000,  2139,  6442,  3085,\n",
            "          2104,  4942, 28689, 14413,  1010,  1010,  1010,  2030,  1997,  2930,\n",
            "         23297,  1012,  1000,  1012,  4087, 16051,  1012,  1011,  1011, 12114,\n",
            "          2025,  3395,  2000,  3622, 15973, 12546,  1012,  1011,  1011,  2930,\n",
            "         16345,  1997,  1996,  7521,  1998, 10662,  2552,  1006,  1022,  1057,\n",
            "          1012,  1055,  1012,  1039,  1012, 10630,  2487,  1007,  2003, 13266,\n",
            "          2000,  3191,  2004,  4076,  1024,  1000, 12114,  2040,  2024,  4914,\n",
            "          2000,  1996,  2142,  2163,  2004,  4568,  3901,  2104,  2930, 19843,\n",
            "          2030, 18512,  2030,  3005,  3570,  2003, 10426,  2104,  1000,  1012,\n",
            "          2731,  1012,  1011,  1011,  2930, 19843,  1997,  2107,  2552,  1006,\n",
            "          1022,  1057,  1012,  1055,  1012,  1039,  1012, 10630,  2581,  1007,\n",
            "          2003, 13266,  2011,  8478,  1000,  4905,  2236,  1000,  1998, 19274,\n",
            "          2075,  1000,  3187,  1997, 10759,  3036,  1000,  1012,  2795,  1997,\n",
            "          8417,  1012,  1011,  1011,  1996,  2795,  1997,  8417,  2005,  2107,\n",
            "          2552,  2003, 13266,  2011,  8478,  1996,  8875,  8800,  2000,  2930,\n",
            "         19348,  1998, 19274,  2075,  1996,  2206,  1024,  1000,  3949,  1997,\n",
            "         12114,  4914,  2004,  8711,  1998, 12114,  4379, 11386,  1012,  1000,\n",
            "          1012, 10995,  8910,  1012,  1011,  1011,  1999,  2236,  1012,  1011,\n",
            "          1011,  2498,  1999,  1996, 16051,  2081,  2011,  2023,  2930,  2089,\n",
            "          2022,  9530,  3367, 28551,  2000,  5787,  3229,  2000,  1996,  6666,\n",
            "          2649,  2012,  3127,  1016,  1997,  2516,  4921,  1997,  1996,  7521,\n",
            "          1998, 10662,  2552,  1006,  1022,  1057,  1012,  1055,  1012,  1039,\n",
            "          1012, 15017,  2487,  3802,  7367,  4160,  1012,  1007,  1012, 18856,\n",
            "          8486, 10803,  1012,  1011,  1011, 12114,  4914,  2005, 26410,  4568,\n",
            "          5039,  2104,  2930, 19843,  2030, 18512,  1997,  1996,  7521,  1998,\n",
            "         10662,  2552,  1006,  1022,  1057,  1012,  1055,  1012,  1039,  1012,\n",
            "         10630,  2581,  1025, 10630,  2620,  1007,  2030,  2040, 14171,  3570,\n",
            "          2000, 26410,  4568,  6319,  2104,  2930, 19348,  1997,  2107,  2552,\n",
            "          4618,  2022,  2641,  2000,  2022,  8711,  1998, 12114,  4379, 11386,\n",
            "          1999, 10388,  2007,  5433, 28048,  1010, 28203,  1010, 25873,  1010,\n",
            "          1998,  4724,  2487,  1997,  1996,  3167,  5368,  1998,  2147,  4495,\n",
            "         16088,  2552,  1997,  2727,  1006,  1022,  1057,  1012,  1055,  1012,\n",
            "          1039,  1012, 29186,  1025, 17365,  2509,  1025, 28133,  1025, 25702,\n",
            "          1007,  1012,  4621,  3058,  1012,  1011,  1011,  2023,  2930,  1010,\n",
            "          1998,  1996, 16051,  2081,  2011,  2023,  2930,  1010,  4618,  2468,\n",
            "          4621,  2006,  1996,  3041,  1997,  1011,  1011,  1996,  3058,  2008,\n",
            "          2003,  8380,  2420,  2044,  1996,  3058,  1997,  1996, 26465,  1997,\n",
            "          2023,  2552,  1025,  2030,  1996,  3058,  2006,  2029,  1037,  2345,\n",
            "          3627,  2003, 20877,  5313, 11644,  2000, 10408,  2023,  2930,  1012,\n",
            "         10651,  1997,  7684,  1998, 11073,  8624,  1012,  2927,  2007, 10807,\n",
            "          2095, 12609,  1010,  1996,  3187,  1997,  2110,  4618,  1010,  2043,\n",
            "          4292,  1996,  3815,  1997,  1996,  7684,  1998, 11073,  8624,  2005,\n",
            "          8711,  1010,  5676,  2008,  1011,  1011,  1996,  3946,  3815,  2003,\n",
            "         10426,  2061,  2008,  2009,  2003, 11706,  2000,  3073,  2005,  1996,\n",
            "         11436,  3988, 25141, 24007,  3791,  1997,  8711,  1010,  2164, 19158,\n",
            "          1996,  3815,  2005, 14200,  1998,  1996,  3465,  1997,  2542,  1025,\n",
            "          1996,  3831, 10817,  1997,  1996,  3946,  2003,  3024,  2012,  1996,\n",
            "          2927,  1997,  1996, 10807,  2095,  2000,  2169,  2120, 25141, 24007,\n",
            "          4034,  2008,  2003,  7182,  2000,  5676, 11706,  2334,  1998,  2120,\n",
            "          3977,  2000,  3710,  1996,  3988, 25141, 24007,  3791,  1997,  8711,\n",
            "          1996,   102]])\n",
            "Segment 5, start_token 2550, end_token 3060\n",
            "101 102\n",
            "tensor([[  101,  3187,  3424,  6895, 17585,  2015,  1996,  4034,  2097, 25141,\n",
            "          9286,  2802,  1996, 10807,  2095,  1025,  1998,  3176,  8310,  2024,\n",
            "          3024,  2000,  2169,  2120, 25141, 24007,  4034, 13364,  2588,  1996,\n",
            "          5508,  1997,  8711,  2008,  1010,  7262,  1997,  1996,  8310,  3024,\n",
            "         27081,  2000, 20423,  1010,  2024,  7182,  2000,  3113,  1996, 11436,\n",
            "          3988, 25141, 24007,  3791,  1997,  2107,  8711,  1998,  2490,  2334,\n",
            "          1998,  2120,  6515,  5366,  1999,  9987,  1997,  1996, 10035,  2649,\n",
            "          1999, 20423,  1012, 12016,  1997, 13141,  2565,  6736,  1012,  2009,\n",
            "          2003,  1996,  3168,  1997,  1996,  3519,  2008,  1011,  1011,  1996,\n",
            "          2343,  2323, 16823,  1037,  2317,  2160, 10669,  2006, 13141,  3860,\n",
            "          1998,  3946,  2107,  2880,  1996,  3691,  1998,  3095,  4072,  2000,\n",
            "         13530,  1010,  3188, 25090,  4371,  1010,  1998,  2599,  4073,  2000,\n",
            "          4769, 13141,  3860,  3314,  2008,  9125,  3674,  6736,  1010,  2164,\n",
            "          1996, 13141, 20247,  2565,  1010,  1998,  2000, 10663,  6970,  4270,\n",
            "          9407,  5966,  1999,  1037, 23259,  1010,  8114,  1010,  1998,  4621,\n",
            "          5450,  1025,  1998,  2023,  2597,  2323,  2022,  2012,  1037,  3026,\n",
            "          2504,  1998,  5478,  2004,  1037,  4650,  2005,  6098,  1037,  3278,\n",
            "          2504,  1997,  3188,  3325,  1999,  1996, 13141,  3860,  2492,  1012,\n",
            "          2553,  2968,  1012,  1999,  2236,  1012,  1011,  1011,  5069,  1997,\n",
            "          3946,  2565,  1012,  1011,  1011,  1996,  2472,  1997,  1996,  2436,\n",
            "          1997, 13141, 25141, 24007,  4618,  2191,  8624,  2000,  2120, 25141,\n",
            "         24007,  6736,  2000,  5452,  1037,  2553,  2968,  2291,  2000,  6509,\n",
            "          4591,  3633,  1999,  3229,  2075,  2578,  1010,  6666,  1010,  1998,\n",
            "          5375,  2005,  2029,  2027,  2024,  7792,  2008,  2024,  3024,  2011,\n",
            "          1011,  1011,  1996,  2436,  1997, 13141, 25141, 24007,  1006,  1996,\n",
            "          1000,  2436,  1000,  1007,  1025,  2060,  2976,  1010,  2110,  1010,\n",
            "          2030,  2334,  6736,  1025,  1998,  2797,  2030, 14495,  4411,  1012,\n",
            "          4591,  3633,  1012,  1011,  1011,  3395,  2000, 20423,  2015,  1998,\n",
            "          1010,  2151,  3265,  2040,  2001,  2012,  2151,  2051,  7792,  2005,\n",
            "         25141, 24007,  1010, 16222, 11314, 18924,  1010,  2030, 27667,  2578,\n",
            "          3024,  2011,  1996,  2436,  4618,  2022,  4591,  2000,  4374,  1996,\n",
            "          2553,  2968,  2578,  2649,  1999, 20423,  1012,  2558,  1997,  8263,\n",
            "          1012,  1011,  1011,  3272,  2004,  3024,  1999, 20423,  1010,  2019,\n",
            "          3265,  2649,  1999, 20423,  4618,  2022,  4591,  2000,  4374,  1996,\n",
            "          2553,  2968,  2578,  2649,  1999, 20423,  2076,  1996,  2558,  2927,\n",
            "          2006,  1996,  3058,  2107,  3265,  2001,  4340,  2000,  2022,  7792,\n",
            "          2005, 25141, 24007,  1010, 16222, 11314, 18924,  1010,  2030, 27667,\n",
            "          2578,  3024,  2011,  1996,  2436,  1010,  1998,  5719,  2005,  1015,\n",
            "          2095,  2044,  1996,  3058,  2006,  2029,  2107,  3265, 13236,  2015,\n",
            "          2000,  2022,  7792,  2005,  2107, 25141, 24007,  1010, 16222, 11314,\n",
            "         18924,  1010,  2030, 27667,  2326,  1012, 11790,  2005, 11813,  6214,\n",
            "          1012,  1011,  1011,  1999,  2236,  1012,  1011,  1011, 26206, 20423,\n",
            "          1010,  2019,  3265,  2649,  1999,  4942, 28689, 14413,  4618,  2022,\n",
            "          4591,  2000,  4374,  1996,  2553,  2968,  2578,  2649,  1999, 20423,\n",
            "          2076,  1996,  2558,  2927,  2006,  1996,  3058,  2006,  2029,  2107,\n",
            "          3265,  2001,  4340,  7792,  2005, 25141, 24007,  1010, 16222, 11314,\n",
            "         18924,  1010,  2030, 27667,  2578,  3024,  2011,  1996,  2436,  1010,\n",
            "          1998,  5719,  2005,  1017,  2086,  2044,  1996,  3058,  2006,  2029,\n",
            "          2107,  3265, 13236,  2015,  2000,  2022,  7792,  2005,  2107, 25141,\n",
            "         24007,  1010, 16222, 11314, 18924,  1010,  2030, 27667,  2578,  1012,\n",
            "         11813,  6214,  1012,  1011,  1011,  2019,  3265,  2649,  1999, 20423,\n",
            "          2089,   102]])\n",
            "Segment 6, start_token 3060, end_token 3570\n",
            "101 102\n",
            "tensor([[  101,  2022,  5845,  1999, 10388,  2007,  4942, 28689, 14413,  2065,\n",
            "          2107,  3265,  1011,  1011,  2003,  9750,  1025,  2038,  9313, 25141,\n",
            "         24007,  2030, 16222, 11314, 18924,  3791,  2008, 17727, 14728,  2107,\n",
            "          3265,  1005,  1055,  3754,  2000,  6162, 25634,  2969,  1011, 10514,\n",
            "         26989, 29125,  1025,  2003,  1037, 13141,  2040,  2001, 25141, 14782,\n",
            "          2013,  1037,  3663,  1997,  4013, 24301, 13508,  1025,  2003,  1037,\n",
            "          2266,  1997,  1037,  2155, 11922,  2005,  2019, 14477,  5946,  7690,\n",
            "         13141,  3576,  1025,  2030,  2012,  1996,  2051,  1997,  4443,  1011,\n",
            "          1011,  2018,  1037, 11980,  2030,  3809,  2966,  4650,  1025,  2018,\n",
            "          5177,  2740,  3785,  1025,  2001,  2112,  1997,  1037,  4398,  3753,\n",
            "          2011,  1037,  2309,  6687,  1025,  2030,  2018,  2042,  1996,  6778,\n",
            "          1997,  1037,  5729,  2433,  1997,  4808,  1012,  6210,  1997, 25141,\n",
            "         24007,  1010, 16222, 11314, 18924,  1010,  2030, 27667,  2578,  1012,\n",
            "          1011,  1011,  2005,  1996,  5682,  1997,  2023,  2930,  1010,  1996,\n",
            "          2744,  1010,  1000, 25141, 24007,  1010, 16222, 11314, 18924,  1010,\n",
            "          2030, 27667,  2578,  1000,  4618,  2421,  2035,  1997,  1996,  2578,\n",
            "          3024,  2011,  1996,  2436,  2000, 12114,  1010,  2007,  1996,  6453,\n",
            "          1997,  1996,  2553,  2968,  2578,  3024,  2104, 20423,  1012, 20104,\n",
            "          1997, 22713,  1012,  1011,  1011,  2045,  2003,  9362,  2000,  2022,\n",
            "         29223,  2107, 20571,  2004,  2089,  2022,  4072,  2000,  4287,  2041,\n",
            "          2023,  2930,  1012, 10995, 11075,  1012,  1011,  1011,  2498,  1999,\n",
            "          2023,  2930,  4618,  2022,  9530,  3367, 28551,  2004, 12473,  1996,\n",
            "          3691,  1997,  1996,  2472,  2104,  2930, 25873,  1997,  1996,  7521,\n",
            "          1998, 10662,  2552,  1006,  1022,  1057,  1012,  1055,  1012,  1039,\n",
            "          1012, 15017,  2475,  1007,  1010,  2030,  1997,  2151,  2060,  2930,\n",
            "          1997,  2107,  2552,  1010,  2000,  3073,  2553,  2968,  2578,  2000,\n",
            "          3633,  2040,  2031,  2042,  1999,  1996,  2142,  2163,  2005,  2936,\n",
            "          2084,  4029,  2706,  1012,  3623,  1999,  5356, 10504,  1012,  1999,\n",
            "          2236,  1012,  1011,  1011,  2930, 25873,  1997,  1996,  7521,  1998,\n",
            "         10662,  2552,  2003, 13266,  1011,  1011,  1999,  4942, 29015,  1010,\n",
            "          2011,  5815,  2012,  1996,  2203,  1996,  2206,  1024,  1000,  3395,\n",
            "          2000,  1996, 11343,  1997,  5029, 29223,  2005,  2023,  3800,  1010,\n",
            "          5375,  1998,  2591,  2578,  2005,  6107,  1010,  2740,  1998,  2542,\n",
            "         11727,  4618,  2022,  2800,  2005,  1037,  2558,  1997,  2025,  2625,\n",
            "          2084,  2260,  2706,  1012,  1000,  1025,  1999,  4942, 29015,  1010,\n",
            "          2011,  5815,  2012,  1996,  2203,  1996,  2206,  1024,  1000,  3395,\n",
            "          2000,  1996, 11343,  1997,  5029, 29223,  2005,  2023,  3800,  1010,\n",
            "          5375,  1998,  2578,  4618,  2022,  2081,  2800,  2000,  8711,  2005,\n",
            "          1037,  2558,  1997,  2025,  2625,  2084,  2260,  2706,  1012,  1000,\n",
            "          1025,  1998,  1999,  4942, 29015,  1011,  1011,  2011,  8478,  1000,\n",
            "          1000,  1998, 19274,  2075,  1000,  1000,  1025,  1998,  2011,  5815,\n",
            "          2012,  1996,  2203,  1996,  2206,  1024,  1000,  3395,  2000,  1996,\n",
            "         11343,  1997,  5029, 29223,  2005,  2023,  3800,  1010,  2107,  5375,\n",
            "          4618,  2022,  3024,  2005,  1037,  6263,  1997,  2260,  2706,  2927,\n",
            "          2007,  1996,  2034,  3204,  1999,  2029,  2107, 13141,  3133,  1996,\n",
            "          2142,  2163,  1012,  1000,  1012,  4621,  3058,  1012,  1011,  1011,\n",
            "          1996, 16051,  2081,  2011,  4942, 29015,  4618,  2468,  4621,  2006,\n",
            "          1996,  3041,  1997,  1011,  1011,  1996,  2034,  2154,  1997,  1996,\n",
            "          2034, 10807,  2095,  2008,  4269,  2044,  1996,  3058,  1997,  1996,\n",
            "         26465,  1997,  2023,  2552,  1025,  2030,  1996,  3058,  2006,  2029,\n",
            "          1037,  2345,  3627,  2003, 20877,  5313, 11644,  2000, 10408,  2023,\n",
            "          2930,   102]])\n",
            "Segment 7, start_token 3570, end_token 4080\n",
            "101 102\n",
            "tensor([[  101,  1012,  3627,  1997,  2810,  1012,  1011,  1011,  1996,  2260,\n",
            "          1011,  3204, 12827,  4838,  1999,  1996, 16051,  2081,  2011,  4942,\n",
            "         29015,  2003,  1037,  6263,  3115,  2005,  1996,  9347,  1997,  2578,\n",
            "          2000,  1996, 13141,  2451,  1012,  2053,  2112,  1997,  2023,  2552,\n",
            "          4618,  2022, 10009,  2004, 14879,  2030,  8161,  5375,  2525,  3024,\n",
            "          2005,  1037,  2558,  2936,  2084,  2260,  2706,  1012, 13141,  8346,\n",
            "          8624,  1012,  5069,  1997, 13141,  8346,  8624,  1012,  1011,  1011,\n",
            "          1996,  2472,  1997,  1996,  2436,  1997, 13141, 25141, 24007,  2003,\n",
            "          9362,  2000,  2400,  8624,  2000,  2451,  1011,  2241,  4411,  1010,\n",
            "          2120, 14495,  4411,  2383,  3325,  1998, 11532,  1999,  7521,  2375,\n",
            "          1998,  1996,  3423,  1010,  2591,  1010,  4547,  1010,  2030,  3451,\n",
            "          3791,  1997,  7489,  1010,  1998,  2120, 25141, 24007,  6736,  2005,\n",
            "          1996,  2640,  1998,  7375,  1997,  3454,  2000,  3749,  2731,  1998,\n",
            "         10296,  2000, 24159,  2000,  1996,  2142,  2163,  2000,  6509,  2068,\n",
            "          1999, 22380,  2046,  1996,  8388,  2166,  1997,  1996,  2142,  2163,\n",
            "          1012,  2578,  6787,  1012,  1011,  1011,  3454,  6787,  2104,  4942,\n",
            "         29015,  4618,  2022,  2109,  1011,  1011,  2000,  3749,  5375,  1998,\n",
            "          7899,  2000, 12114,  2649,  1999,  4942, 29015,  1010,  2006,  1011,\n",
            "          1011,  1996,  3019,  3989,  2832,  1025,  1996,  3423,  5918,  2005,\n",
            "          3019,  3989,  1025,  2916,  1998, 10198,  1997,  1057,  1012,  1055,\n",
            "          1012,  4480,  1025,  3451, 10296,  1025,  2394,  2004,  1037,  2117,\n",
            "          2653,  1025,  8388,  2015,  1998,  2142,  2163,  2381,  1025,  3847,\n",
            "          1025,  5193,  1025, 28667,  8743,  9031,  1025,  6107,  2731,  1025,\n",
            "          5177,  2740,  2578,  1025,  2775, 16302,  2578,  1025,  1998,  2060,\n",
            "          7832,  1996,  2472, 14847,  2004, 24791,  1996, 10296,  1998, 19037,\n",
            "          1997,  2047, 25470,  2000,  1996,  2142,  2163,  1025,  2000,  3749,\n",
            "         10296,  1998,  5375,  2000,  2976,  1010,  2110,  1010,  1998,  2334,\n",
            "          4896,  2008,  3710, 24159,  2000,  1996,  2142,  2163,  2061,  2004,\n",
            "          2000,  6509,  2107,  4896,  1999,  3116,  2037,  3791,  1025,  1998,\n",
            "          2000,  3073,  4087,  5375,  1998,  2731,  2000,  2451,  1011,  2241,\n",
            "          4411,  4346,  7899,  1999,  2394,  2004,  1037,  2117,  2653,  1010,\n",
            "          8388,  2015,  1010,  1998,  2142,  2163,  2381,  1998,  4087,  5375,\n",
            "          1998,  2731,  2000,  2451,  1011,  2241,  4411,  4346,  3019,  3989,\n",
            "         15641,  1998,  4646,  5375,  1012,  7792,  5381,  1012,  1011,  1011,\n",
            "          5381,  7792,  2005,  5375,  1998,  7899,  2104,  2023,  4942, 29015,\n",
            "          2421,  1011,  1011, 12114,  4914,  2004,  8711,  2104,  2930, 19843,\n",
            "          1997,  1996,  7521,  1998, 10662,  2552,  1025, 12114,  4379, 11386,\n",
            "          2104,  2930, 18512,  1997,  2107,  2552,  1025, 12114,  4914,  2004,\n",
            "          2569,  7489,  2013,  5712,  2030,  7041, 27081,  2000,  2930, 13412,\n",
            "          2549,  1997,  1996,  2120,  3639, 20104,  2552,  2005, 10807,  2095,\n",
            "          2263,  1006,  2270,  2375,  7287,  1011, 18596,  1025, 13092, 28093,\n",
            "          1012,  4464,  2575,  1007,  1025,  2151,  7344,  2040,  2001,  2012,\n",
            "          2151,  2051,  7792,  2000,  4374,  2578,  2013,  1996,  2436,  1997,\n",
            "         13141, 25141, 24007,  1025,  1998,  2060, 12114,  4351,  2011,  1996,\n",
            "          2472,  1997,  1996,  2436,  1997, 13141, 25141, 24007,  1012,  4646,\n",
            "          2005,  8624,  1012,  1011,  1011,  2169,  9178,  4078, 24771,  1037,\n",
            "         13141,  8346,  3946,  2104,  2023,  2930,  4618, 12040,  2019,  4646,\n",
            "          2000,  1996,  2472,  2012,  2107,  2051,  1998,  1999,  2107,  5450,\n",
            "          2004,  1996,  2472,  2089,  5478,  1012,  9470,  1012,  1011,  1011,\n",
            "          1999, 17739,  3946, 10285,  1010,  1996,  2472,  4618,  2507,  9470,\n",
            "          2000,  4411,  2593,  2007,  3325,  1999, 25141, 15073,  8711,  2030,\n",
            "          1999,   102]])\n",
            "Segment 8, start_token 4080, end_token 4590\n",
            "101 102\n",
            "tensor([[  101, 28965,  2578,  2005,  8711,  1998, 11386, 24071,  1012,  4311,\n",
            "          2000,  3519,  1012,  1011,  1011,  2927,  8380,  2420,  2044,  1996,\n",
            "          3058,  1997,  1996, 26465,  1997,  2023,  2552,  1010,  1998,  6604,\n",
            "          6920,  1010,  1996,  2472,  4618,  3189,  2000,  1996, 14814,  9528,\n",
            "          1997,  1996,  4001,  1998,  1996,  2160,  1997,  4505,  1037,  3189,\n",
            "          2006,  1996,  7375,  1997,  2023,  2930,  1012,  1996,  3189,  4618,\n",
            "          2421,  2592,  4953,  1996,  8624,  3843, 27081,  2000,  2023,  2930,\n",
            "          1998,  1996,  3463,  1997,  2216,  8624,  1012,  9844,  3946,  2565,\n",
            "          4935,  1012,  2224,  1997,  1996, 13141,  9844,  3946,  2565,  1012,\n",
            "          1011,  1011,  1999,  2236,  1012,  1011,  1011,  1999,  5038,  1997,\n",
            "          1996,  3893, 13105,  4719,  2005,  3633,  2366,  2083,  1996, 13141,\n",
            "          9844,  3946,  2565,  1010,  2104,  2029,  2976,  5029,  2024, 10349,\n",
            "          2011,  2797,  4753,  5857,  1997,  5356,  1010,  5350,  1010,  1998,\n",
            "          7314,  1010,  1996,  2472,  1997,  1996,  2436,  1997, 13141, 25141,\n",
            "         24007,  4618,  5676,  2008,  1996, 11207,  2015,  2649,  1999, 20423,\n",
            "          2024,  2777,  2007,  4847,  2000,  6577,  2011,  7792,  3633,  1999,\n",
            "          2107,  2565,  1012,  3296, 11207,  2015,  1012,  1011,  1011,  1999,\n",
            "          2236,  1012,  1011,  1011,  3395,  2000,  1996, 11343,  1997,  5029,\n",
            "         29223,  2005,  2023,  3800,  1010,  1996,  7017,  1997,  7792,  3633,\n",
            "          2040,  6611,  2005,  2578,  2104,  1996,  9844,  3946,  2565,  2040,\n",
            "          4618,  2022,  2366,  2083,  2107,  2565,  2003,  2275,  2012,  1996,\n",
            "          2206,  3798,  1999,  2169,  1997,  1996,  2206, 10807,  2086,  1011,\n",
            "          1011,  2012,  2560,  3438,  3867,  1999, 10807,  2095, 12609,  1025,\n",
            "          2012,  2560,  3963,  3867,  1999, 10807,  2095, 25682,  1025,  1998,\n",
            "          2012,  2560,  3770,  3867,  1999, 10807,  2095, 16798,  2475,  1010,\n",
            "          1998,  6920,  1012, 11790,  1012,  1011,  1011,  1999, 12515,  1996,\n",
            "         11207,  2015,  1999,  2023, 20423,  1010,  1996,  2472,  4618, 23329,\n",
            "          3633,  2040,  2024,  9776,  1010,  2104,  1996,  2287,  1997,  2324,\n",
            "          1010,  2030,  2058,  1996,  2287,  1997,  3515,  4983,  2107,  3633,\n",
            "          2024,  2108,  2366,  2004,  2372,  1997,  2019,  7792,  2155,  1012,\n",
            "          3091,  1997, 11395,  2005,  3946,  1012,  1011,  1011,  3633,  2004,\n",
            "          2649,  1999, 20423,  4618,  2022,  7792,  2005,  2578,  2104,  1996,\n",
            "          9844,  3946,  2565,  1010,  1998,  3568,  2022,  3517,  2000,  6162,\n",
            "          2969,  1011, 10514, 26989, 29125,  1010,  2306,  8380,  2000, 11212,\n",
            "          2420,  1997,  2037, 10316,  1999,  1996,  2565,  1012,  3815,  1997,\n",
            "          3946,  1012,  1011,  1011,  3296, 10651,  1012,  1011,  1011,  3395,\n",
            "          2000,  1996, 11343,  1997,  5029, 29223,  2005,  2023,  3800,  1010,\n",
            "          2927,  2007, 10807,  2095, 12609,  1010,  1996,  2472,  4618,  1010,\n",
            "          2043,  4292,  1996,  3815,  1997,  1996,  9844,  8624,  1010,  5676,\n",
            "          2008,  1996,  3946,  3815,  2003, 10426,  6604,  2061,  2008,  2009,\n",
            "          2003, 11706,  2000,  3073,  2005,  2578,  3832,  2000,  2393,  8711,\n",
            "          2468,  2969,  1011,  7182,  2306,  8380,  2000, 11212,  2420,  1010,\n",
            "          2164, 19158,  1996,  3815,  2005, 14200,  1998,  1996,  3465,  1997,\n",
            "          2542,  1012,  7667,  1012,  1011,  1011,  2025,  2101,  2084,  2382,\n",
            "          2420,  2044,  1996,  3058,  1997, 26465,  1997,  2023,  2552,  1010,\n",
            "          1996,  2472,  4618, 16617,  2019,  7667,  1997,  1996,  2566,  8353,\n",
            "          3815,  2008,  2003,  3024,  1999,  1996,  9844,  3946,  2565,  1012,\n",
            "          1996,  7667,  4618,  1011,  1011, 11628,  1996,  4748,  2063, 16211,\n",
            "          5666,  1997,  1996,  2783,  2566,  8353,  3946,  3815,  2000,  3073,\n",
            "          1996,  5375,  4072,  2000,  9585,  3633,  2040,  2024,  2366,  2011,\n",
            "          1996,  2565,  2000,  6162,  2969,  1011, 10514, 26989, 29125,  2306,\n",
            "         11212,   102]])\n",
            "Segment 9, start_token 4590, end_token 5100\n",
            "101 102\n",
            "tensor([[  101,  2420,  1025,  5646,  1996,  2566,  8353,  3946,  3815,  2008,\n",
            "          2052,  2022,  4072,  1999,  2344,  2000,  3073,  1996,  5375,  4072,\n",
            "          2000,  9585,  3633,  2040,  2024,  2366,  2011,  1996,  2565,  6162,\n",
            "          2969,  1011, 10514, 26989, 29125,  2306, 11212,  2420,  1025,  1998,\n",
            "         14358,  1996,  4022,  4254,  1997,  2151,  2060,  3431,  2000,  1996,\n",
            "          3169,  1997,  1996,  2565,  2008,  1996,  2472,  2030,  1996,  2436,\n",
            "          1005,  1055,  2512,  1011,  2976,  5826,  6592,  2000,  5335,  1996,\n",
            "         12353,  1997,  1996,  2565,  1012, 16053,  2007,  5826,  1012,  1011,\n",
            "          1011,  1999,  9283,  1996,  7667,  3615,  2000,  1999,  4942, 28689,\n",
            "         14413,  1010,  1996,  2472,  4618, 23363,  2007,  4505,  1997,  2120,\n",
            "          1998,  2334, 25141, 24007,  6736,  2007,  3325,  1999,  4346,  2578,\n",
            "          2104,  1996,  9844,  3946,  2565,  2000,  4553,  2037,  5328,  2006,\n",
            "          1996,  2565,  1010,  2164,  2037,  5328,  2007,  4847,  2000, 24059,\n",
            "          2083,  1997,  4942, 28689, 14413,  1012,  3189,  2000,  3519,  1012,\n",
            "          1011,  1011,  2025,  2101,  2084,  6036,  2420,  2044,  1996,  3058,\n",
            "          1997, 26465,  1997,  2023,  2552,  1010,  1996,  2472,  4618,  3189,\n",
            "          2000,  1996,  6413,  9528,  1997,  3519,  1996,  3463,  1997,  1996,\n",
            "          7667,  2081, 27081,  2000,  4942, 28689, 14413,  1012,  2107,  3189,\n",
            "          4618,  2421,  1011,  1011,  1996,  2472,  1005,  1055,  9556, 27081,\n",
            "          2000,  4942, 28689, 14413,  1025,  1996,  3815,  1997,  1996,  2566,\n",
            "          8353,  3946,  2008,  2097,  2022,  3024,  2927,  2006,  1996,  2034,\n",
            "          2154,  1997,  1996,  2034, 10807,  2095,  2008,  4269,  2044,  1996,\n",
            "          3058,  1997, 26465,  1997,  2023,  2552,  1025,  1998,  1037, 12654,\n",
            "          1997,  1996,  5328,  5228,  2011,  1996, 11422,  2649,  1999,  4942,\n",
            "         28689, 14413,  1012,  3946,  3815,  1999,  6438,  1997,  3189,  1012,\n",
            "          1011,  1011,  2065,  2005,  2151,  3114,  1996,  3189,  2649,  1999,\n",
            "          4942, 28689, 14413,  2003,  2025,  6406,  2007,  1996,  6413,  9528,\n",
            "          1997,  1996,  3519,  2006,  2030,  2077,  1996,  3223,  3058,  2005,\n",
            "          1996, 12339,  1997,  2107,  3189,  1010,  1996,  2566,  8353,  9844,\n",
            "          3946,  3815,  2005,  1996,  2034, 10807,  2095,  2008,  4269,  2044,\n",
            "          1996,  3058,  1997, 26465,  1997,  2023,  2552,  4618,  2022,  2275,\n",
            "          2012,  1996,  3815,  2275,  2005,  2107,  3946,  2006,  2244,  2382,\n",
            "          1997,  1996,  3025, 10807,  2095,  1998,  4013,  9250,  2000,  8339,\n",
            "          1037,  8380,  1011,  2000, 11212,  1011,  2154,  3946,  2558,  1012,\n",
            "          4968,  5057, 13141, 25141, 24007,  4636,  1012,  1999,  2236,  1012,\n",
            "          1011,  1011,  2045,  2003,  2511,  1037,  4968,  5057, 13141, 25141,\n",
            "         24007,  4636,  2008,  4618,  2022,  2800,  2000,  1996,  2472,  1997,\n",
            "          1996,  2436,  1997, 13141, 25141, 24007,  2000,  3113, 14477, 16778,\n",
            "          6895, 17585,  2094, 25141, 24007,  3791,  1012,  4009,  7698,  1997,\n",
            "          5029,  1012,  1011,  1011,  7188,  1996,  2472, 16463,  2009,  2000,\n",
            "          2022,  2590,  2000,  1996,  2120,  3037,  1010,  1996,  2472,  2003,\n",
            "          9362,  2000,  6519, 24014,  2006,  2107,  3408,  1998,  3785,  2004,\n",
            "          1996,  2472,  2089,  5646,  1010,  5375,  2104,  2023,  3127,  2005,\n",
            "          1996,  3800,  1997,  3116,  9223, 13661, 25141, 24007,  3791,  1997,\n",
            "          7080,  1998,  4279,  2366, 27081,  2000,  2023,  3127,  1010,  2164,\n",
            "          4279, 13417,  2152,  3616,  1997, 25470,  1997,  8711,  2349,  2000,\n",
            "          3905,  9230,  1012,  3499,  3085,  3594,  1012,  1011,  1011,  1996,\n",
            "          2472,  2089,  6519, 24014,  5375,  2000,  7792, 11422,  2005,  1996,\n",
            "          5682,  1997,  4346, 25141, 24007,  5375,  2000,  8711,  1010,  2164,\n",
            "          5193,  1010,  3847,  1010,  6107,  1010,  2740,  1010,  5177,  2740,\n",
            "          1010,  2394,  2653,  2731,  2578,  1010, 28667,  8743,  9031,  1010,\n",
            "          1998,   102]])\n",
            "Segment 10, start_token 5100, end_token 5610\n",
            "101 102\n",
            "tensor([[  101,  2060,  5682,  2004,  4340,  2011,  1996,  2472,  1012,  7792,\n",
            "         11422,  4225,  1012,  1011,  1011,  2120, 25141, 24007,  6736,  1010,\n",
            "          2451,  1011,  2241,  4411,  1010,  8639, 12084, 11252,  6401,  1998,\n",
            "          3454,  1010,  1998,  2163,  1012,  7740, 26828,  1012,  1011,  1011,\n",
            "          2053,  8310,  2089,  2022,  9633,  2013,  1996,  4636,  9362,  2011,\n",
            "          4942, 29015,  4983,  1996,  2472,  2038,  4340,  2008,  1037,  4968,\n",
            "         25141, 24007,  5057,  6526,  1998,  2038,  7378,  2107,  2755,  2000,\n",
            "          6413,  9528,  1997,  3519,  1012,  6178,  2006,  4636,  1012,  1011,\n",
            "          1011,  2053,  5029,  2089,  2022, 29223,  2029,  2043,  2794,  2000,\n",
            "          1996,  8310,  3130, 29223,  2021,  2025,  2664, 27885, 14715,  3064,\n",
            "          1010,  2052,  3426,  2107,  8310,  2000, 13467,  1002,  2531,  1010,\n",
            "          2199,  1010,  2199,  6604,  1012, 20104,  1997, 22713,  1012,  1011,\n",
            "          1011,  5029, 29223, 27081,  2000,  2023,  2930,  4618,  2022,  2800,\n",
            "          2127,  4654, 11837,  5732,  1012, 27024,  3036,  3318,  6666,  1012,\n",
            "          2930, 28048,  1997,  1996,  3167,  5368,  1998,  2147,  4495, 16088,\n",
            "          2552,  1997,  2727,  2003, 13266,  1011,  1011,  2011, 27950,  2075,\n",
            "          4942, 28689, 14413,  2000,  3191,  2004,  4076,  1024,  1000,  6453,\n",
            "          2005,  8711,  1998,  2004, 12844,  2229,  1012,  1011,  1011,  2007,\n",
            "          4847,  2000,  1996,  9675,  2976,  3454,  2649,  1999, 20423,  1010,\n",
            "         20423,  4618,  2025,  6611,  2000,  1011,  1011,  1000,  2019,  7344,\n",
            "          4914,  2000,  1996,  2142,  2163,  2004,  1037, 13141,  2104,  2930,\n",
            "         19843,  1997,  1996,  7521,  1998, 10662,  2552,  1025,  1000,  2019,\n",
            "          7344,  4379, 11386,  2104,  2930, 18512,  1997,  2107,  2552,  1025,\n",
            "          1000,  2019,  7344,  3005, 23702,  2003,  2007, 24850,  2104,  2930,\n",
            "         22884,  1997,  2107,  2552,  1006,  2004,  1999,  3466,  3202,  2077,\n",
            "          1996,  4621,  3058,  1997,  2930, 24559,  1997,  2407,  1039,  1997,\n",
            "          2270,  2375,  9645,  1011, 18512,  1007,  2030,  2930, 22343,  1997,\n",
            "          2107,  2552,  1006,  2004, 13266,  2011,  2930, 20405,  1997,  2407,\n",
            "          1039,  1997,  2270,  2375,  9645,  1011, 18512,  1007,  1025,  1000,\n",
            "          2019,  7344,  2040,  2003,  4379,  3570,  2004,  1037,  9642,  1998,\n",
            "         21404,  4372,  6494,  3372,  1006,  2004,  4225,  1999,  2930, 16202,\n",
            "          1997,  1996, 13141,  2495,  5375,  2552,  1997,  3150,  1007,  1025,\n",
            "          1000,  2019,  7344,  4914,  2004,  1037,  2569, 11560,  2013,  5712,\n",
            "          2030,  7041, 27081,  2000,  2930, 13412,  2549,  1997,  1996,  2120,\n",
            "          3639, 20104,  2552,  2005, 10807,  2095,  2263,  1025,  2030,  1000,\n",
            "          2019,  7344,  2040,  2003,  4914,  2000,  1996,  2142,  2163,  2004,\n",
            "          2019,  2572, 24140,  2937, 11560, 27081,  2000,  2930,  5388,  2549,\n",
            "          1997,  1996,  3097,  3136,  1010,  9167, 12135,  1010,  1998,  3141,\n",
            "          3454, 22713,  2552,  1010,  2997,  1006,  2004,  4838,  1999,  2930,\n",
            "          7886,  1997,  2270,  2375,  2531,  1011, 16798,  1998, 13266,  2011,\n",
            "          1996,  6280,  4013, 11365,  2080,  2104,  9230,  1998, 13141,  5375,\n",
            "          1999,  2516,  2462,  1997,  1996,  3097,  3136,  1010,  9167, 12135,\n",
            "          1010,  1998,  3141,  3454, 22713,  2552,  1010,  2960,  1010,  2270,\n",
            "          2375,  2531,  1011,  4805,  2487,  1010,  2004, 13266,  1007,  1012,\n",
            "          1000,  1025,  1998,  2011,  5815,  2012,  1996,  2203,  1996,  2206,\n",
            "          1024,  1000,  7020,  2072,  6666,  2005,  3056, 12114,  1998,  5694,\n",
            "          1997, 11626,  1012,  1011,  1011,  2927,  2006,  1996,  3058,  1997,\n",
            "          1996, 26465,  1997,  2023,  4942, 28689, 14413,  1010,  2151,  4591,\n",
            "          7344,  1006,  2004,  4225,  1999,  2930,  4724,  2487,  1007,  1010,\n",
            "          6778,  1997, 11626,  1999,  5381,  1006,  2004,  4225,  1999,  2930,\n",
            "         10550,  1997,  2407,  1037,  1997,  1996,  5694,  1997, 11626,  1998,\n",
            "          4808,   102]])\n",
            "Segment 11, start_token 5610, end_token 5924\n",
            "101 102\n",
            "tensor([[  101,  3860,  2552,  1997,  2456,  1006,  2570,  1057,  1012,  1055,\n",
            "          1012,  1039,  1012, 27671,  2629,  1007,  1007,  1010,  2030,  7344,\n",
            "          4379,  3570,  2104,  2930,  7886,  1997,  1996,  7521,  1998, 10662,\n",
            "          2552,  1006,  1022,  1057,  1012,  1055,  1012,  1039,  1012,  7287,\n",
            "          2487,  1007, 10155, 22023,  2005,  1996,  9675,  2976,  2565,  2649,\n",
            "          1999, 20423,  9578,  2011,  3114,  1997,  1996, 18287,  1997,  1996,\n",
            "          1021,  1011,  2095,  2558,  2649,  1999,  4942, 28689, 14413,  3188,\n",
            "          2000,  1996, 26465,  1997,  2023,  4942, 28689, 14413,  4618,  2022,\n",
            "          7792,  2005,  6666,  2104,  2107,  2565,  2302,  7634,  2000,  4942,\n",
            "         28689, 14413,  1012,  1000,  1012,  2437,  2569, 11560, 11799,  3570,\n",
            "          3841, 12879, 24108,  5134,  1998, 14477, 21408,  8737,  7088,  2098,\n",
            "          2336,  4379,  1000,  1057,  1000,  9425,  3860,  7792,  2005,  6469,\n",
            "          2729,  1998, 13141,  6666,  1012, 11395,  2005,  6666,  1998,  2578,\n",
            "          1012,  1011,  1011,  2930, 10550,  1997,  1996,  5694,  1997, 11626,\n",
            "          1998,  4808,  3860,  2552,  1997,  2456,  1006,  2570,  1057,  1012,\n",
            "          1055,  1012,  1039,  1012, 27671,  2629,  1007,  2003, 13266,  2011,\n",
            "         19274,  2075,  1000,  1010,  2019,  7344,  2040,  2038,  2042,  4379,\n",
            "          2569, 11560,  3570,  2104,  2930,  7886,  1997,  1996,  7521,  1998,\n",
            "         10662,  2552,  1006,  1022,  1057,  1012,  1055,  1012,  1039,  1012,\n",
            "          7287,  2487,  1007,  1010,  1037,  2775,  2040,  2038,  2042,  4379,\n",
            "          3570,  2104,  2930,  7886,  1997,  1996,  7521,  1998, 10662,  2552,\n",
            "          1006,  1022,  1057,  1012,  1055,  1012,  1039,  1012,  7287,  2487,\n",
            "          1006,  2321,  1007,  1010,  1000,  2044,  1000, 11626,  1999,  5381,\n",
            "          1010,  1000,  1012,  2110, 24964, 14905, 28393,  3672,  1012,  1011,\n",
            "          1011,  3395,  2000,  1996, 11343,  1997,  5029, 29223,  2005,  2023,\n",
            "          3800,  1010,  2065,  2110,  6469,  2729,  5029,  2024,  4654, 11837,\n",
            "          5732,  2006,  6852,  1997,  1037,  2775,  2040,  2038,  2042,  4379,\n",
            "          2569, 11560,  3570,  2104,  2930,  7886,  1997,  1996,  7521,  1998,\n",
            "         10662,  2552,  1006,  1022,  1057,  1012,  1055,  1012,  1039,  1012,\n",
            "          7287,  2487,  1007,  1010,  1996,  2976,  2231,  4618, 24964, 14905,\n",
            "         28393,  1996,  2110,  1999,  2029,  1996,  2775, 11665,  2005,  2107,\n",
            "         22697,  2011,  1996,  2110,  1012,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,   102]])\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.bert_embed_single_row(a.practice_bill.loc[:, 'cleaned_text'][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
